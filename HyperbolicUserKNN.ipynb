{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/chiranjivdas09/ta-feng-grocery-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"chiranjivdas09/ta-feng-grocery-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV_MODE: False MAX_USERS: None\n",
      "Shape: (817741, 9)\n",
      "Columns: ['TRANSACTION_DT', 'CUSTOMER_ID', 'AGE_GROUP', 'PIN_CODE', 'PRODUCT_SUBCLASS', 'PRODUCT_ID', 'AMOUNT', 'ASSET', 'SALES_PRICE']\n",
      "Detected columns: {'user_col': 'CUSTOMER_ID', 'item_col': 'PRODUCT_ID', 'date_col': 'TRANSACTION_DT'}\n"
     ]
    }
   ],
   "source": [
    "# Импорты и глобальная конфигурация\n",
    "import os, glob, math, pickle, time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- Fast dev mode ---\n",
    "DEV_MODE = False\n",
    "MAX_USERS = 8000 if DEV_MODE else None  # None для полного прогона\n",
    "\n",
    "MIN_BASKETS_PER_USER = 3  # должно быть >= 3, чтобы можно было сделать train/val/test\n",
    "\n",
    "# ---Метрики и размер списка рекомендаций---\n",
    "TOPK_LIST = [5, 10, 20]\n",
    "TOPN_RECOMMEND = 200  # внутренняя длина списка кандидатов (сколько объектов ранжируем)\n",
    "\n",
    "\n",
    "# --- UserKNN ---\n",
    "USERKNN_TUNE_GRID = [50, 100, 200, 500]  # значения числа соседей для подбора\n",
    "USERKNN_DEFAULT_K = 200\n",
    "\n",
    "# --- TIFU-KNN(simple) ---\n",
    "TIFU_GROUPS_GRID = [5, 7]              # варианты числа групп истории\n",
    "TIFU_ALPHA_GRID = [0.5, 0.7, 0.9]      # варианты смешивания PIF/IU\n",
    "TIFU_NEIGHBORS_GRID = [100, 300]       # варианты числа соседей\n",
    "\n",
    "# --- ItemKNN (добавляем, чтобы \"KNN baseline\" был однозначно покрыт) ---\n",
    "ITEMKNN_TUNE_GRID = [50, 100, 200]   # сколько похожих items хранить на item (topK)\n",
    "ITEMKNN_DEFAULT_K = 100\n",
    "\n",
    "\n",
    "TIFU_WITHIN_DECAY = 0.9  # затухание внутри группы\n",
    "TIFU_GROUP_DECAY = 0.7   # затухание между группами\n",
    "TIFU_DEFAULT_GROUPS = 7\n",
    "TIFU_DEFAULT_ALPHA = 0.7\n",
    "TIFU_DEFAULT_K = 300\n",
    "\n",
    "print(\"DEV_MODE:\", DEV_MODE, \"MAX_USERS:\", MAX_USERS)\n",
    "\n",
    "def find_csv_candidate():\n",
    "    # Ищем CSV-файлы в папке Kaggle input\n",
    "    cands = glob.glob('/kaggle/input/*/*.csv') + glob.glob('/kaggle/input/*/*.CSV')\n",
    "    if not cands:\n",
    "        raise FileNotFoundError('В /kaggle/input не найдены CSV. Проверьте, что датасет добавлен в ноутбук.')\n",
    "\n",
    "    # Предпочитаем файл, который похож на Ta-Feng по названию\n",
    "    for p in cands:\n",
    "        low = p.lower()\n",
    "        if ('ta' in low and 'feng' in low) or ('tafeng' in low):\n",
    "            return p\n",
    "\n",
    "    # Если не нашли — берём самый большой CSV (как запасной вариант)\n",
    "    cands = sorted(cands, key=lambda p: os.path.getsize(p), reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def pick(candidates):\n",
    "        for c in candidates:\n",
    "            if c in cols:\n",
    "                return cols[c]\n",
    "        return None\n",
    "\n",
    "    user_col = pick(['customer_id', 'cust_id', 'user_id', 'userid', 'member_id', 'client_id'])\n",
    "    item_col = pick(['product_id', 'item_id', 'prod_id', 'sku_id', 'article_id'])\n",
    "    date_col = pick(['transaction_dt', 'trans_date', 'date', 't_dat', 'datetime', 'transaction_date'])\n",
    "\n",
    "    # В Ta-Feng часто встречаются имена в верхнем регистре: CUSTOMER_ID, PRODUCT_ID, TRANSACTION_DT\n",
    "    if user_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'CUSTOMER_ID':\n",
    "                user_col = c\n",
    "                break\n",
    "    if item_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'PRODUCT_ID':\n",
    "                item_col = c\n",
    "                break\n",
    "    if date_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'TRANSACTION_DT':\n",
    "                date_col = c\n",
    "                break\n",
    "\n",
    "    return user_col, item_col, date_col\n",
    "\n",
    "df = pd.read_csv('/root/.cache/kagglehub/datasets/chiranjivdas09/ta-feng-grocery-dataset/versions/1/ta_feng_all_months_merged.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns)[:30])\n",
    "\n",
    "user_col, item_col, date_col = detect_columns(df)\n",
    "print(\"Detected columns:\", {\"user_col\": user_col, \"item_col\": item_col, \"date_col\": date_col})\n",
    "\n",
    "if user_col is None or item_col is None or date_col is None:\n",
    "    raise ValueError(\n",
    "        \"Не удалось автоматически определить необходимые колонки. \"\n",
    "        \"Пожалуйста, задайте user_col/item_col/date_col вручную после просмотра df.columns.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baskets: (119578, 3) Unique users: 32266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_raw</th>\n",
       "      <th>date</th>\n",
       "      <th>items_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-03</td>\n",
       "      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-05</td>\n",
       "      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-19</td>\n",
       "      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-28</td>\n",
       "      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-12-02</td>\n",
       "      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_raw       date                                          items_raw\n",
       "0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...\n",
       "1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...\n",
       "2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...\n",
       "3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...\n",
       "4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Приводим типы к строкам (важно, чтобы не потерять ведущие нули в идентификаторах)\n",
    "df[user_col] = df[user_col].astype(str)\n",
    "df[item_col] = df[item_col].astype(str)\n",
    "\n",
    "# Парсим дату, некорректные строки превращаются в NaT\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "# Удаляем строки без даты/пользователя/товара\n",
    "df = df.dropna(subset=[date_col, user_col, item_col]).copy()\n",
    "\n",
    "# Округляем datetime вниз до даты (без времени)\n",
    "df['__date'] = df[date_col].dt.floor('D')\n",
    "\n",
    "# Группировка в корзины\n",
    "basket_df = (\n",
    "    df.groupby([user_col, '__date'])[item_col]\n",
    "      .apply(lambda s: list(pd.unique(s)))   # уникальные товары в корзине\n",
    "      .reset_index()\n",
    "      .rename(columns={user_col: 'user_raw', '__date': 'date', item_col: 'items_raw'})\n",
    ")\n",
    "\n",
    "basket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\n",
    "print(\"Baskets:\", basket_df.shape, \"Unique users:\", basket_df['user_raw'].nunique())\n",
    "basket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter/dev: baskets (95072, 3) users 14074\n",
      "n_users: 14074 n_items: 22817\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_raw</th>\n",
       "      <th>date</th>\n",
       "      <th>items_raw</th>\n",
       "      <th>u</th>\n",
       "      <th>item_idx_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-03</td>\n",
       "      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-05</td>\n",
       "      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-19</td>\n",
       "      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-28</td>\n",
       "      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-12-02</td>\n",
       "      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[32, 39, 20, 40, 41]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_raw       date                                          items_raw  u  \\\n",
       "0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...  0   \n",
       "1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...  0   \n",
       "2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...  0   \n",
       "3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...  0   \n",
       "4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ...  0   \n",
       "\n",
       "                                      item_idx_list  \n",
       "0                                [0, 1, 2, 3, 4, 5]  \n",
       "1              [6, 7, 8, 9, 10, 11, 12, 13, 14, 15]  \n",
       "2  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]  \n",
       "3      [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]  \n",
       "4                              [32, 39, 20, 40, 41]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Фильтруем пользователей с достаточным числом корзин:\n",
    "#    нам нужно минимум 3 корзины на пользователя, чтобы сформировать train/val/test\n",
    "counts = basket_df.groupby('user_raw').size()\n",
    "keep_users = counts[counts >= MIN_BASKETS_PER_USER].index\n",
    "basket_df = basket_df[basket_df['user_raw'].isin(keep_users)].copy()\n",
    "\n",
    "# Опционально: режим разработки (dev).\n",
    "#    Если задан MAX_USERS, берём только первых N пользователей (после сортировки/порядка появления).\n",
    "#    Это ускоряет эксперименты и отладку в Kaggle.\n",
    "if MAX_USERS is not None:\n",
    "    users = basket_df['user_raw'].unique()[:MAX_USERS]\n",
    "    basket_df = basket_df[basket_df['user_raw'].isin(users)].copy()\n",
    "\n",
    "# На всякий случай пересортируем по пользователю и времени,\n",
    "#    чтобы дальнейший split по времени был корректным\n",
    "basket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\n",
    "print(\"After filter/dev: baskets\", basket_df.shape, \"users\", basket_df['user_raw'].nunique())\n",
    "\n",
    "# Маппинг сырого user/item ID в индексы 0..n-1\n",
    "#    Это нужно для эффективной работы с матрицами (scipy.sparse) и моделями.\n",
    "user_ids = basket_df['user_raw'].unique()\n",
    "item_ids = pd.unique(np.concatenate(basket_df['items_raw'].values))\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(user_ids)}\n",
    "item2idx = {it: i for i, it in enumerate(item_ids)}\n",
    "\n",
    "# обратные отображения (удобно для дебага/вывода рекомендаций)\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "# 5) Добавляем индекс пользователя и переводим списки товаров в список индексов\n",
    "basket_df['u'] = basket_df['user_raw'].map(user2idx)\n",
    "basket_df['item_idx_list'] = basket_df['items_raw'].apply(lambda xs: [item2idx[x] for x in xs])\n",
    "\n",
    "n_users = len(user2idx)\n",
    "n_items = len(item2idx)\n",
    "print(\"n_users:\", n_users, \"n_items:\", n_items)\n",
    "\n",
    "display(basket_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with train/val/test: 14074\n"
     ]
    }
   ],
   "source": [
    "user_baskets = defaultdict(list)  # u -> list of (date, [items])\n",
    "for row in basket_df.itertuples(index=False):\n",
    "    user_baskets[row.u].append((row.date, row.item_idx_list))\n",
    "\n",
    "# Гарантируем сортировку по времени (вдруг где-то нарушилась)\n",
    "for u in user_baskets:\n",
    "    user_baskets[u] = sorted(user_baskets[u], key=lambda x: x[0])\n",
    "\n",
    "train_hist = {}\n",
    "val_basket = {}\n",
    "test_basket = {}\n",
    "\n",
    "for u, seq in user_baskets.items():\n",
    "    baskets = [b for _, b in seq]\n",
    "    # На всякий случай проверяем минимальную длину\n",
    "    if len(baskets) < 3:\n",
    "        continue\n",
    "    train_hist[u] = baskets[:-2]   # все корзины, кроме двух последних\n",
    "    val_basket[u]  = baskets[-2]   # предпоследняя корзина\n",
    "    test_basket[u] = baskets[-1]   # последняя корзина\n",
    "\n",
    "print(\"Users with train/val/test:\", len(train_hist))\n",
    "assert len(train_hist) > 0, \"No users available after filtering/splitting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_raw nnz: 383239 density: 0.001193\n"
     ]
    }
   ],
   "source": [
    "# Строим user-item матрицу только по train_hist (без val/test), чтобы избежать утечки будущего.\n",
    "# X_raw[u, it] = сколько train-корзин пользователя u содержали товар it (presence in basket).\n",
    "rows, cols, data = [], [], []\n",
    "for u, baskets in train_hist.items():\n",
    "    c = Counter()\n",
    "    for b in baskets:\n",
    "        for it in set(b):   # presence in basket: учитываем товар один раз на корзину\n",
    "            c[it] += 1\n",
    "    for it, v in c.items():\n",
    "        rows.append(u)\n",
    "        cols.append(it)\n",
    "        data.append(float(v))\n",
    "\n",
    "X_raw = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n",
    "\n",
    "# L2-нормировка по пользователям для косинусной похожести:\n",
    "# cos(u,v) = dot(X_cos[u], X_cos[v])\n",
    "X_cos = normalize(X_raw, norm='l2', axis=1)\n",
    "\n",
    "# Разреженность матрицы: доля ненулевых элементов\n",
    "density = X_raw.nnz / (n_users * n_items)\n",
    "print(\"X_raw nnz:\", X_raw.nnz, \"density:\", f\"{density:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def build_x_bin_from_xraw(X_raw_csr):\n",
    "    Xb = X_raw_csr.copy().tocsr()\n",
    "    Xb.data = np.ones_like(Xb.data, dtype=np.float32)\n",
    "    return Xb\n",
    "\n",
    "def topk_sorted_csr(mat_csr, k):\n",
    "    \"\"\"\n",
    "    Оставляет top-k элементов по значению в каждой строке CSR и сортирует их по убыванию.\n",
    "    \"\"\"\n",
    "    mat = mat_csr.tocsr()\n",
    "    indptr, indices, data = mat.indptr, mat.indices, mat.data\n",
    "\n",
    "    new_indptr = np.zeros(mat.shape[0] + 1, dtype=np.int32)\n",
    "    new_indices = []\n",
    "    new_data = []\n",
    "\n",
    "    nnz_so_far = 0\n",
    "    for i in range(mat.shape[0]):\n",
    "        start, end = indptr[i], indptr[i + 1]\n",
    "        row_idx = indices[start:end]\n",
    "        row_data = data[start:end]\n",
    "\n",
    "        if row_data.size == 0:\n",
    "            new_indptr[i + 1] = nnz_so_far\n",
    "            continue\n",
    "\n",
    "        if row_data.size > k:\n",
    "            top = np.argpartition(-row_data, k)[:k]\n",
    "            top = top[np.argsort(-row_data[top])]\n",
    "            row_idx = row_idx[top]\n",
    "            row_data = row_data[top]\n",
    "        else:\n",
    "            order = np.argsort(-row_data)\n",
    "            row_idx = row_idx[order]\n",
    "            row_data = row_data[order]\n",
    "\n",
    "        new_indices.extend(row_idx.tolist())\n",
    "        new_data.extend(row_data.astype(np.float32).tolist())\n",
    "        nnz_so_far += len(row_idx)\n",
    "        new_indptr[i + 1] = nnz_so_far\n",
    "\n",
    "    return sparse.csr_matrix(\n",
    "        (np.array(new_data, dtype=np.float32),\n",
    "         np.array(new_indices, dtype=np.int32),\n",
    "         new_indptr),\n",
    "        shape=mat.shape\n",
    "    )\n",
    "\n",
    "def build_item_cosine_sim_topk(X_raw_csr, topk=100, use_binary=True):\n",
    "    \"\"\"\n",
    "    Строим item-item cosine similarity из train-матрицы:\n",
    "    - X_bin: user×item (0/1)\n",
    "    - C = X_bin.T @ X_bin: item×item co-occurrence по пользователям\n",
    "    - cosine: C_ij / (||i|| * ||j||)\n",
    "    - оставляем topk соседей на item\n",
    "    \"\"\"\n",
    "    Xb = build_x_bin_from_xraw(X_raw_csr) if use_binary else X_raw_csr.tocsr()\n",
    "    Xi = Xb.T.tocsr()  # item×user\n",
    "\n",
    "    norms = np.sqrt(np.asarray(Xi.multiply(Xi).sum(axis=1)).ravel()) + 1e-12\n",
    "\n",
    "    C = (Xi @ Xi.T).tocsr()\n",
    "    C.setdiag(0.0)\n",
    "    C.eliminate_zeros()\n",
    "\n",
    "    C = C.tocoo()\n",
    "    C.data = (C.data / (norms[C.row] * norms[C.col])).astype(np.float32)\n",
    "\n",
    "    S = C.tocsr()\n",
    "    S = topk_sorted_csr(S, topk)\n",
    "    return S\n",
    "\n",
    "def userknn_recommender_factory(X_train_csr, S_useruser_csr, fallback_scores=None):\n",
    "    n_items_local = X_train_csr.shape[1]\n",
    "\n",
    "    if fallback_scores is None:\n",
    "        fallback_scores = np.asarray(X_train_csr.sum(axis=0)).ravel().astype(np.float32)\n",
    "    else:\n",
    "        fallback_scores = np.asarray(fallback_scores).ravel().astype(np.float32)\n",
    "\n",
    "    def recommend(u, topn=TOPN_RECOMMEND):\n",
    "        sim_users = S_useruser_csr[u] \n",
    "        \n",
    "        tmp = sim_users.dot(X_train_csr)\n",
    "\n",
    "        if tmp.nnz == 0:\n",
    "            scores = fallback_scores\n",
    "        else:\n",
    "            tmp = tmp.tocsr()\n",
    "            scores = np.zeros(n_items_local, dtype=np.float32)\n",
    "            scores[tmp.indices] = tmp.data.astype(np.float32)\n",
    "            \n",
    "\n",
    "        topn2 = min(topn, scores.shape[0])\n",
    "        idx = np.argpartition(-scores, topn2)[:topn2]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "        return idx.tolist()\n",
    "\n",
    "    return recommend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num user edges: 19195608\n"
     ]
    }
   ],
   "source": [
    "def build_user_user_graph(R, tau=2):\n",
    "    W = (R @ R.T).tocoo()\n",
    "    \n",
    "    mask = (W.row != W.col) & (W.data >= tau)\n",
    "    edges = np.vstack([W.row[mask], W.col[mask]]).T\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "user_edges = build_user_user_graph(X_raw, tau=2)\n",
    "print(f\"Num user edges: {len(user_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "class PoincareEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_items, dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_items, dim)\n",
    "        nn.init.uniform_(self.emb.weight, -1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.project_to_ball(self.emb(idx))\n",
    "    \n",
    "    def project_to_ball(self, x, eps=EPS):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "        max_norm = 1 - eps\n",
    "        return x / norm.clamp_min(EPS) * torch.clamp(norm, max=max_norm)\n",
    "    \n",
    "    def poincare_distance(self, x, y):\n",
    "        x2 = (x * x).sum(dim=-1)\n",
    "        y2 = (y * y).sum(dim=-1)\n",
    "        diff2 = ((x - y) ** 2).sum(dim=-1)\n",
    "        denom = (1 - x2) * (1 - y2)\n",
    "        z = 1 + 2 * diff2 / denom.clamp_min(EPS)\n",
    "        return torch.acosh(z.clamp_min(1 + EPS))\n",
    "    \n",
    "    def sample_negatives(self, batch_size, num_items, K):\n",
    "        return torch.randint(low=0, high=num_items, size=(batch_size, K))\n",
    "    \n",
    "    def poincare_loss(self, i, j, negs):\n",
    "        xi = self.forward(i)\n",
    "        xj = self.forward(j)\n",
    "        xk = self.forward(negs)\n",
    "    \n",
    "        d_pos = self.poincare_distance(xi, xj)\n",
    "        d_neg = self.poincare_distance(xi.unsqueeze(1), xk)\n",
    "    \n",
    "        numerator = torch.exp(-d_pos)\n",
    "        denominator = torch.exp(-d_neg).sum(dim=1)\n",
    "    \n",
    "        loss = -torch.log(numerator / denominator.clamp_min(EPS))\n",
    "        return loss.mean()\n",
    "    \n",
    "    def train_embedding(self, edges, num_items, optimizer, epochs=10, batch_size=256, neg_k=10, device=\"cpu\"):\n",
    "        self.to(device)\n",
    "        edges = torch.tensor(edges, dtype=torch.long, device=device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            perm = torch.randperm(len(edges), device=device)\n",
    "            total_loss = 0.0\n",
    "    \n",
    "            for idx in trange(0, len(edges), batch_size):\n",
    "                batch_idx = perm[idx:idx + batch_size]\n",
    "                batch = edges[batch_idx]\n",
    "    \n",
    "                i = batch[:, 0]\n",
    "                j = batch[:, 1]\n",
    "                negs = self.sample_negatives(len(i), num_items, neg_k).to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss = self.poincare_loss(i, j, negs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    self.emb.weight.copy_(self.project_to_ball(self.emb.weight))\n",
    "    \n",
    "                total_loss += loss.item() * len(i)\n",
    "    \n",
    "            print(f\"Epoch {epoch+1}: loss = {total_loss / len(edges):.4f}\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:20<00:00, 133.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 2.1108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:27<00:00, 127.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 2.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:33<00:00, 122.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 2.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:31<00:00, 123.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 2.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:34<00:00, 121.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 2.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 2.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 119.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 2.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 2.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 2.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:34<00:00, 121.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 2.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 119.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss = 2.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:33<00:00, 121.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss = 2.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:34<00:00, 121.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss = 2.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:32<00:00, 122.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss = 2.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss = 2.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:33<00:00, 122.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss = 2.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 119.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss = 2.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss = 2.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 120.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss = 2.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:37<00:00, 119.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss = 2.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: loss = 2.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:34<00:00, 121.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: loss = 2.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: loss = 2.0235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:34<00:00, 121.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: loss = 2.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 119.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: loss = 2.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:36<00:00, 119.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: loss = 2.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: loss = 2.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:33<00:00, 122.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: loss = 2.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: loss = 2.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18746/18746 [02:35<00:00, 120.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: loss = 2.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PoincareEmbedding(\n",
       "  (emb): Embedding(14074, 30)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = X_raw.shape[0]\n",
    "dim = 30\n",
    "lr = 0.001\n",
    "\n",
    "user_model = PoincareEmbedding(num_users, dim)\n",
    "user_optimizer = torch.optim.Adam(user_model.parameters(), lr=lr)\n",
    "\n",
    "user_model.train_embedding(\n",
    "    edges=user_edges,\n",
    "    num_items=num_users,  \n",
    "    optimizer=user_optimizer,\n",
    "    epochs=30,             \n",
    "    batch_size=1024,\n",
    "    neg_k=15,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "def poincare_distance(x, y):\n",
    "    \"\"\"\n",
    "    Стандартное расстояние в шаре Пуанкаре\n",
    "    x, y: (..., d)\n",
    "    \"\"\"\n",
    "    x2 = (x * x).sum(dim=-1)\n",
    "    y2 = (y * y).sum(dim=-1)\n",
    "    diff2 = ((x - y) ** 2).sum(dim=-1)\n",
    "    denom = (1 - x2) * (1 - y2)\n",
    "    z = 1 + 2 * diff2 / denom.clamp_min(EPS)\n",
    "    return torch.acosh(z.clamp_min(1 + EPS))\n",
    "\n",
    "\n",
    "def hyperbolic_cosine_distance(x, y):\n",
    "    \"\"\"\n",
    "    Гиперболический аналог косинусного сходства:\n",
    "    dist = 1 - <x, y>_L / (||x||_L * ||y||_L)\n",
    "    В простейшей форме используем обычный dot, нормируем по норме в шаре.\n",
    "    \"\"\"\n",
    "    x_norm = torch.norm(x, dim=-1, keepdim=True).clamp_min(EPS)\n",
    "    y_norm = torch.norm(y, dim=-1, keepdim=True).clamp_min(EPS)\n",
    "    cos_sim = (x @ y.transpose(-2, -1)) / (x_norm * y_norm.transpose(-2, -1))\n",
    "    return 1 - cos_sim\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(((x - y) ** 2).sum(dim=-1))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pairwise_distance(emb, metric='poincare'):\n",
    "    \"\"\"\n",
    "    Возвращает матрицу попарных расстояний для всех эмбеддингов.\n",
    "    \"\"\"\n",
    "    x = emb.unsqueeze(1)  # (n,1,d)\n",
    "    y = emb.unsqueeze(0)  # (1,n,d)\n",
    "    \n",
    "    if metric == 'poincare':\n",
    "        return poincare_distance(x, y)\n",
    "    elif metric == 'hyperbolic_cosine':\n",
    "        return hyperbolic_cosine_distance(x, y)\n",
    "    elif metric == 'euclidean':\n",
    "        return euclidean_distance(x, y)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def knn_hyperbolic(model, k=10, metric='poincare'):\n",
    "    emb = model.emb.weight\n",
    "    dist = pairwise_distance(emb, metric=metric)\n",
    "    knn = torch.topk(dist, k=k+1, largest=False).indices[:, 1:]\n",
    "    return knn\n",
    "\n",
    "\n",
    "def build_user_hyperbolic_knn(model, topk=100, device='cpu', metric='poincare'):\n",
    "    with torch.no_grad():\n",
    "        emb = model.emb.weight.to(device)\n",
    "        n = emb.shape[0]\n",
    "\n",
    "        rows, cols, data = [], [], []\n",
    "\n",
    "        for i in range(n):\n",
    "            xi = emb[i].unsqueeze(0)\n",
    "            if metric == 'poincare':\n",
    "                d = poincare_distance(xi, emb).cpu().numpy()\n",
    "            elif metric == 'hyperbolic_cosine':\n",
    "                d = hyperbolic_cosine_distance(xi, emb).cpu().numpy().flatten()\n",
    "            elif metric == 'euclidean':\n",
    "                d = euclidean_distance(xi, emb).cpu().numpy().flatten()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "            d[i] = np.inf\n",
    "            idx = np.argpartition(d, topk)[:topk]\n",
    "            idx = idx[np.argsort(d[idx])]\n",
    "\n",
    "            rows.extend([i] * len(idx))\n",
    "            cols.extend(idx.tolist())\n",
    "            data.extend(np.exp(-d[idx]).astype(np.float32))\n",
    "\n",
    "    return sparse.csr_matrix(\n",
    "        (data, (rows, cols)),\n",
    "        shape=(n, n)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_knn = build_user_hyperbolic_knn(user_model, topk=1000, metric='poincare')\n",
    "S_knn_cos = build_user_hyperbolic_knn(user_model, topk=1000, metric='hyperbolic_cosine')\n",
    "S_knn_euc = build_user_hyperbolic_knn(user_model, topk=1000, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "userknn_hyper_rec = userknn_recommender_factory(X_raw, S_knn_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(pred_items, true_items, k):\n",
    "    \"\"\"\n",
    "    Recall@K = доля товаров из истинной корзины, которые попали в top-K рекомендаций.\n",
    "    true_items: список товаров в истинной корзине (val/test)\n",
    "    pred_items: ранжированный список рекомендаций\n",
    "    \"\"\"\n",
    "    pred_k = pred_items[:k]\n",
    "    true_set = set(true_items)\n",
    "    if len(true_set) == 0:\n",
    "        return 0.0\n",
    "    return len(set(pred_k) & true_set) / len(true_set)\n",
    "\n",
    "def ndcg_at_k(pred_items, true_items, k):\n",
    "    \"\"\"\n",
    "    NDCG@K учитывает порядок: попадания в верхние позиции оцениваются выше.\n",
    "    Здесь релевантность бинарная: товар релевантен, если он есть в true_items.\n",
    "    \"\"\"\n",
    "    true_set = set(true_items)\n",
    "    pred_k = pred_items[:k]\n",
    "\n",
    "    # DCG\n",
    "    dcg = 0.0\n",
    "    for i, it in enumerate(pred_k):\n",
    "        if it in true_set:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # i=0 -> log2(2)=1\n",
    "\n",
    "    # IDCG: максимум возможного DCG при идеальном ранжировании\n",
    "    ideal_hits = min(k, len(true_set))\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_model(recommender_fn, users, true_baskets, topk_list=(5,10,20)):\n",
    "    \"\"\"\n",
    "    Оцениваем модель на пользователях:\n",
    "    - recommender_fn(u) должен возвращать ранжированный список item_id (индексы товаров)\n",
    "    - true_baskets[u] — истинная корзина (список item_id)\n",
    "    Возвращаем средние Recall@K и NDCG@K по пользователям для каждого K.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        u = int(u)\n",
    "        pred = recommender_fn(u)\n",
    "        true = true_baskets[u]\n",
    "        for k in topk_list:\n",
    "            rows.append({\n",
    "                \"u\": u,\n",
    "                \"k\": int(k),\n",
    "                \"recall\": recall_at_k(pred, true, k),\n",
    "                \"ndcg\": ndcg_at_k(pred, true, k),\n",
    "            })\n",
    "\n",
    "    return (pd.DataFrame(rows)\n",
    "            .groupby(\"k\")[[\"recall\",\"ndcg\"]].mean()\n",
    "            .reset_index())\n",
    "\n",
    "def tag_result(df_res, model_name, split_name):\n",
    "    \"\"\"Добавляем метаданные (название модели и сплит) к таблице метрик.\"\"\"\n",
    "    out = df_res.copy()\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"split\"] = split_name\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>recall</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.048595</td>\n",
       "      <td>0.059343</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.062983</td>\n",
       "      <td>0.059842</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.082810</td>\n",
       "      <td>0.065296</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k    recall      ndcg                     model split\n",
       "0   5  0.048595  0.059343  UserKNN_Hyper(topk=1000)   val\n",
       "1  10  0.062983  0.059842  UserKNN_Hyper(topk=1000)   val\n",
       "2  20  0.082810  0.065296  UserKNN_Hyper(topk=1000)   val"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "users_val = list(val_basket.keys())\n",
    "res_userknn_hyper_val = evaluate_model(\n",
    "    lambda u: userknn_hyper_rec(u, TOPN_RECOMMEND),\n",
    "    users_val,\n",
    "    val_basket,\n",
    "    TOPK_LIST\n",
    ")\n",
    "\n",
    "val_table_user = pd.concat([\n",
    "    tag_result(res_userknn_hyper_val, f\"UserKNN_Hyper(topk={1000})\", \"val\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "val_table_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>recall</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.069791</td>\n",
       "      <td>0.086440</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.085379</td>\n",
       "      <td>0.085166</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.107093</td>\n",
       "      <td>0.090770</td>\n",
       "      <td>UserKNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k    recall      ndcg                     model split\n",
       "0   5  0.069791  0.086440  UserKNN_Hyper(topk=1000)  test\n",
       "1  10  0.085379  0.085166  UserKNN_Hyper(topk=1000)  test\n",
       "2  20  0.107093  0.090770  UserKNN_Hyper(topk=1000)  test"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userknn_hyper_rec = userknn_recommender_factory(X_raw, S_knn)\n",
    "users_test = list(test_basket.keys())\n",
    "res_userknn_hyper_test = evaluate_model(\n",
    "    lambda u: userknn_hyper_rec(u, TOPN_RECOMMEND),\n",
    "    users_test,\n",
    "    test_basket,\n",
    "    TOPK_LIST\n",
    ")\n",
    "\n",
    "test_table_user = pd.concat([\n",
    "    tag_result(res_userknn_hyper_test, f\"UserKNN_Hyper(topk={1000})\", \"test\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "test_table_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 22987,
     "sourceId": 29446,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
