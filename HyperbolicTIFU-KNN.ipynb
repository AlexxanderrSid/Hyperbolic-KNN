{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV_MODE: False MAX_USERS: None\n",
      "Shape: (817741, 9)\n",
      "Columns: ['TRANSACTION_DT', 'CUSTOMER_ID', 'AGE_GROUP', 'PIN_CODE', 'PRODUCT_SUBCLASS', 'PRODUCT_ID', 'AMOUNT', 'ASSET', 'SALES_PRICE']\n",
      "Detected columns: {'user_col': 'CUSTOMER_ID', 'item_col': 'PRODUCT_ID', 'date_col': 'TRANSACTION_DT'}\n"
     ]
    }
   ],
   "source": [
    "# Импорты и глобальная конфигурация\n",
    "import os, glob, math, pickle, time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- Fast dev mode ---\n",
    "DEV_MODE = False\n",
    "MAX_USERS = 8000 if DEV_MODE else None  # None для полного прогона\n",
    "\n",
    "MIN_BASKETS_PER_USER = 3  # должно быть >= 3, чтобы можно было сделать train/val/test\n",
    "\n",
    "# ---Метрики и размер списка рекомендаций---\n",
    "TOPK_LIST = [5, 10, 20]\n",
    "TOPN_RECOMMEND = 200  # внутренняя длина списка кандидатов (сколько объектов ранжируем)\n",
    "\n",
    "\n",
    "# --- UserKNN ---\n",
    "USERKNN_TUNE_GRID = [50, 100, 200, 500]  # значения числа соседей для подбора\n",
    "USERKNN_DEFAULT_K = 200\n",
    "\n",
    "# --- TIFU-KNN(simple) ---\n",
    "TIFU_GROUPS_GRID = [5, 7]              # варианты числа групп истории\n",
    "TIFU_ALPHA_GRID = [0.5, 0.7, 0.9]      # варианты смешивания PIF/IU\n",
    "TIFU_NEIGHBORS_GRID = [100, 300]       # варианты числа соседей\n",
    "\n",
    "# --- ItemKNN (добавляем, чтобы \"KNN baseline\" был однозначно покрыт) ---\n",
    "ITEMKNN_TUNE_GRID = [50, 100, 200]   # сколько похожих items хранить на item (topK)\n",
    "ITEMKNN_DEFAULT_K = 100\n",
    "\n",
    "\n",
    "TIFU_WITHIN_DECAY = 0.9  # затухание внутри группы\n",
    "TIFU_GROUP_DECAY = 0.7   # затухание между группами\n",
    "TIFU_DEFAULT_GROUPS = 7\n",
    "TIFU_DEFAULT_ALPHA = 0.7\n",
    "TIFU_DEFAULT_K = 300\n",
    "\n",
    "print(\"DEV_MODE:\", DEV_MODE, \"MAX_USERS:\", MAX_USERS)\n",
    "\n",
    "def find_csv_candidate():\n",
    "    # Ищем CSV-файлы в папке Kaggle input\n",
    "    cands = glob.glob('/kaggle/input/*/*.csv') + glob.glob('/kaggle/input/*/*.CSV')\n",
    "    if not cands:\n",
    "        raise FileNotFoundError('В /kaggle/input не найдены CSV. Проверьте, что датасет добавлен в ноутбук.')\n",
    "\n",
    "    # Предпочитаем файл, который похож на Ta-Feng по названию\n",
    "    for p in cands:\n",
    "        low = p.lower()\n",
    "        if ('ta' in low and 'feng' in low) or ('tafeng' in low):\n",
    "            return p\n",
    "\n",
    "    # Если не нашли — берём самый большой CSV (как запасной вариант)\n",
    "    cands = sorted(cands, key=lambda p: os.path.getsize(p), reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def detect_columns(df: pd.DataFrame):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    def pick(candidates):\n",
    "        for c in candidates:\n",
    "            if c in cols:\n",
    "                return cols[c]\n",
    "        return None\n",
    "\n",
    "    user_col = pick(['customer_id', 'cust_id', 'user_id', 'userid', 'member_id', 'client_id'])\n",
    "    item_col = pick(['product_id', 'item_id', 'prod_id', 'sku_id', 'article_id'])\n",
    "    date_col = pick(['transaction_dt', 'trans_date', 'date', 't_dat', 'datetime', 'transaction_date'])\n",
    "\n",
    "    # В Ta-Feng часто встречаются имена в верхнем регистре: CUSTOMER_ID, PRODUCT_ID, TRANSACTION_DT\n",
    "    if user_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'CUSTOMER_ID':\n",
    "                user_col = c\n",
    "                break\n",
    "    if item_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'PRODUCT_ID':\n",
    "                item_col = c\n",
    "                break\n",
    "    if date_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.upper() == 'TRANSACTION_DT':\n",
    "                date_col = c\n",
    "                break\n",
    "\n",
    "    return user_col, item_col, date_col\n",
    "\n",
    "df = pd.read_csv('/root/.cache/kagglehub/datasets/chiranjivdas09/ta-feng-grocery-dataset/versions/1/ta_feng_all_months_merged.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns)[:30])\n",
    "\n",
    "user_col, item_col, date_col = detect_columns(df)\n",
    "print(\"Detected columns:\", {\"user_col\": user_col, \"item_col\": item_col, \"date_col\": date_col})\n",
    "\n",
    "if user_col is None or item_col is None or date_col is None:\n",
    "    raise ValueError(\n",
    "        \"Не удалось автоматически определить необходимые колонки. \"\n",
    "        \"Пожалуйста, задайте user_col/item_col/date_col вручную после просмотра df.columns.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baskets: (119578, 3) Unique users: 32266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_raw</th>\n",
       "      <th>date</th>\n",
       "      <th>items_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-03</td>\n",
       "      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-05</td>\n",
       "      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-19</td>\n",
       "      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-28</td>\n",
       "      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-12-02</td>\n",
       "      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_raw       date                                          items_raw\n",
       "0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...\n",
       "1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...\n",
       "2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...\n",
       "3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...\n",
       "4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Приводим типы к строкам (важно, чтобы не потерять ведущие нули в идентификаторах)\n",
    "df[user_col] = df[user_col].astype(str)\n",
    "df[item_col] = df[item_col].astype(str)\n",
    "\n",
    "# Парсим дату, некорректные строки превращаются в NaT\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "# Удаляем строки без даты/пользователя/товара\n",
    "df = df.dropna(subset=[date_col, user_col, item_col]).copy()\n",
    "\n",
    "# Округляем datetime вниз до даты (без времени)\n",
    "df['__date'] = df[date_col].dt.floor('D')\n",
    "\n",
    "# Группировка в корзины\n",
    "basket_df = (\n",
    "    df.groupby([user_col, '__date'])[item_col]\n",
    "      .apply(lambda s: list(pd.unique(s)))   # уникальные товары в корзине\n",
    "      .reset_index()\n",
    "      .rename(columns={user_col: 'user_raw', '__date': 'date', item_col: 'items_raw'})\n",
    ")\n",
    "\n",
    "basket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\n",
    "print(\"Baskets:\", basket_df.shape, \"Unique users:\", basket_df['user_raw'].nunique())\n",
    "basket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter/dev: baskets (95072, 3) users 14074\n",
      "n_users: 14074 n_items: 22817\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_raw</th>\n",
       "      <th>date</th>\n",
       "      <th>items_raw</th>\n",
       "      <th>u</th>\n",
       "      <th>item_idx_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-03</td>\n",
       "      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-05</td>\n",
       "      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-19</td>\n",
       "      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n",
       "      <td>0</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-11-28</td>\n",
       "      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100021</td>\n",
       "      <td>2000-12-02</td>\n",
       "      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[32, 39, 20, 40, 41]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_raw       date                                          items_raw  u  \\\n",
       "0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...  0   \n",
       "1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...  0   \n",
       "2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...  0   \n",
       "3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...  0   \n",
       "4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ...  0   \n",
       "\n",
       "                                      item_idx_list  \n",
       "0                                [0, 1, 2, 3, 4, 5]  \n",
       "1              [6, 7, 8, 9, 10, 11, 12, 13, 14, 15]  \n",
       "2  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]  \n",
       "3      [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]  \n",
       "4                              [32, 39, 20, 40, 41]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Фильтруем пользователей с достаточным числом корзин:\n",
    "#    нам нужно минимум 3 корзины на пользователя, чтобы сформировать train/val/test\n",
    "counts = basket_df.groupby('user_raw').size()\n",
    "keep_users = counts[counts >= MIN_BASKETS_PER_USER].index\n",
    "basket_df = basket_df[basket_df['user_raw'].isin(keep_users)].copy()\n",
    "\n",
    "# Опционально: режим разработки (dev).\n",
    "#    Если задан MAX_USERS, берём только первых N пользователей (после сортировки/порядка появления).\n",
    "#    Это ускоряет эксперименты и отладку в Kaggle.\n",
    "if MAX_USERS is not None:\n",
    "    users = basket_df['user_raw'].unique()[:MAX_USERS]\n",
    "    basket_df = basket_df[basket_df['user_raw'].isin(users)].copy()\n",
    "\n",
    "# На всякий случай пересортируем по пользователю и времени,\n",
    "#    чтобы дальнейший split по времени был корректным\n",
    "basket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\n",
    "print(\"After filter/dev: baskets\", basket_df.shape, \"users\", basket_df['user_raw'].nunique())\n",
    "\n",
    "# Маппинг сырого user/item ID в индексы 0..n-1\n",
    "#    Это нужно для эффективной работы с матрицами (scipy.sparse) и моделями.\n",
    "user_ids = basket_df['user_raw'].unique()\n",
    "item_ids = pd.unique(np.concatenate(basket_df['items_raw'].values))\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(user_ids)}\n",
    "item2idx = {it: i for i, it in enumerate(item_ids)}\n",
    "\n",
    "# обратные отображения (удобно для дебага/вывода рекомендаций)\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "# 5) Добавляем индекс пользователя и переводим списки товаров в список индексов\n",
    "basket_df['u'] = basket_df['user_raw'].map(user2idx)\n",
    "basket_df['item_idx_list'] = basket_df['items_raw'].apply(lambda xs: [item2idx[x] for x in xs])\n",
    "\n",
    "n_users = len(user2idx)\n",
    "n_items = len(item2idx)\n",
    "print(\"n_users:\", n_users, \"n_items:\", n_items)\n",
    "\n",
    "display(basket_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with train/val/test: 14074\n"
     ]
    }
   ],
   "source": [
    "user_baskets = defaultdict(list)  # u -> list of (date, [items])\n",
    "for row in basket_df.itertuples(index=False):\n",
    "    user_baskets[row.u].append((row.date, row.item_idx_list))\n",
    "\n",
    "# Гарантируем сортировку по времени (вдруг где-то нарушилась)\n",
    "for u in user_baskets:\n",
    "    user_baskets[u] = sorted(user_baskets[u], key=lambda x: x[0])\n",
    "\n",
    "train_hist = {}\n",
    "val_basket = {}\n",
    "test_basket = {}\n",
    "\n",
    "for u, seq in user_baskets.items():\n",
    "    baskets = [b for _, b in seq]\n",
    "    # На всякий случай проверяем минимальную длину\n",
    "    if len(baskets) < 3:\n",
    "        continue\n",
    "    train_hist[u] = baskets[:-2]   # все корзины, кроме двух последних\n",
    "    val_basket[u]  = baskets[-2]   # предпоследняя корзина\n",
    "    test_basket[u] = baskets[-1]   # последняя корзина\n",
    "\n",
    "print(\"Users with train/val/test:\", len(train_hist))\n",
    "assert len(train_hist) > 0, \"No users available after filtering/splitting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_raw nnz: 383239 density: 0.001193\n"
     ]
    }
   ],
   "source": [
    "# Строим user-item матрицу только по train_hist (без val/test), чтобы избежать утечки будущего.\n",
    "# X_raw[u, it] = сколько train-корзин пользователя u содержали товар it (presence in basket).\n",
    "rows, cols, data = [], [], []\n",
    "for u, baskets in train_hist.items():\n",
    "    c = Counter()\n",
    "    for b in baskets:\n",
    "        for it in set(b):   # presence in basket: учитываем товар один раз на корзину\n",
    "            c[it] += 1\n",
    "    for it, v in c.items():\n",
    "        rows.append(u)\n",
    "        cols.append(it)\n",
    "        data.append(float(v))\n",
    "\n",
    "X_raw = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n",
    "\n",
    "# L2-нормировка по пользователям для косинусной похожести:\n",
    "# cos(u,v) = dot(X_cos[u], X_cos[v])\n",
    "X_cos = normalize(X_raw, norm='l2', axis=1)\n",
    "\n",
    "# Разреженность матрицы: доля ненулевых элементов\n",
    "density = X_raw.nnz / (n_users * n_items)\n",
    "print(\"X_raw nnz:\", X_raw.nnz, \"density:\", f\"{density:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building TIFU Matrix: 100%|██████████| 14074/14074 [00:00<00:00, 72334.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_tifu_matrix(train_hist_dict, n_users, n_items, \n",
    "                      groups=TIFU_DEFAULT_GROUPS, \n",
    "                      alpha=TIFU_DEFAULT_ALPHA, \n",
    "                      within_decay=TIFU_WITHIN_DECAY, \n",
    "                      group_decay=TIFU_GROUP_DECAY):\n",
    "    rows, cols, data = [], [], []\n",
    "    \n",
    "    for u, baskets in tqdm(train_hist_dict.items(), desc=\"Building TIFU Matrix\"):\n",
    "        flat_items = [item for basket in baskets for item in basket]\n",
    "        m = len(flat_items)\n",
    "        if m == 0:\n",
    "            continue\n",
    "            \n",
    "        actual_groups = min(groups, m)\n",
    "        group_size = int(np.ceil(m / actual_groups))\n",
    "        \n",
    "        item_scores = defaultdict(float)\n",
    "        \n",
    "        for g in range(actual_groups):\n",
    "            start_idx = g * group_size\n",
    "            end_idx = min((g + 1) * group_size, m)\n",
    "            group_items = flat_items[start_idx:end_idx]\n",
    "            \n",
    "            g_weight = group_decay ** (actual_groups - 1 - g)\n",
    "            \n",
    "            for k, item in enumerate(group_items):\n",
    "                w_weight = within_decay ** (len(group_items) - 1 - k)\n",
    "                \n",
    "                total_weight = g_weight * w_weight\n",
    "                item_scores[item] += total_weight\n",
    "\n",
    "        for item, score in item_scores.items():\n",
    "            rows.append(u)\n",
    "            cols.append(item)\n",
    "            data.append(score)\n",
    "\n",
    "    X_tifu = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n",
    "    X_tifu = normalize(X_tifu, norm='l2', axis=1)\n",
    "    return X_tifu\n",
    "\n",
    "X_tifu = build_tifu_matrix(train_hist, n_users, n_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def build_x_bin_from_xraw(X_raw_csr):\n",
    "    Xb = X_raw_csr.copy().tocsr()\n",
    "    Xb.data = np.ones_like(Xb.data, dtype=np.float32)\n",
    "    return Xb\n",
    "\n",
    "def topk_sorted_csr(mat_csr, k):\n",
    "    \"\"\"\n",
    "    Оставляет top-k элементов по значению в каждой строке CSR и сортирует их по убыванию.\n",
    "    \"\"\"\n",
    "    mat = mat_csr.tocsr()\n",
    "    indptr, indices, data = mat.indptr, mat.indices, mat.data\n",
    "\n",
    "    new_indptr = np.zeros(mat.shape[0] + 1, dtype=np.int32)\n",
    "    new_indices = []\n",
    "    new_data = []\n",
    "\n",
    "    nnz_so_far = 0\n",
    "    for i in range(mat.shape[0]):\n",
    "        start, end = indptr[i], indptr[i + 1]\n",
    "        row_idx = indices[start:end]\n",
    "        row_data = data[start:end]\n",
    "\n",
    "        if row_data.size == 0:\n",
    "            new_indptr[i + 1] = nnz_so_far\n",
    "            continue\n",
    "\n",
    "        if row_data.size > k:\n",
    "            top = np.argpartition(-row_data, k)[:k]\n",
    "            top = top[np.argsort(-row_data[top])]\n",
    "            row_idx = row_idx[top]\n",
    "            row_data = row_data[top]\n",
    "        else:\n",
    "            order = np.argsort(-row_data)\n",
    "            row_idx = row_idx[order]\n",
    "            row_data = row_data[order]\n",
    "\n",
    "        new_indices.extend(row_idx.tolist())\n",
    "        new_data.extend(row_data.astype(np.float32).tolist())\n",
    "        nnz_so_far += len(row_idx)\n",
    "        new_indptr[i + 1] = nnz_so_far\n",
    "\n",
    "    return sparse.csr_matrix(\n",
    "        (np.array(new_data, dtype=np.float32),\n",
    "         np.array(new_indices, dtype=np.int32),\n",
    "         new_indptr),\n",
    "        shape=mat.shape\n",
    "    )\n",
    "\n",
    "def build_item_cosine_sim_topk(X_raw_csr, topk=100, use_binary=True):\n",
    "    \"\"\"\n",
    "    Строим item-item cosine similarity из train-матрицы:\n",
    "    - X_bin: user×item (0/1)\n",
    "    - C = X_bin.T @ X_bin: item×item co-occurrence по пользователям\n",
    "    - cosine: C_ij / (||i|| * ||j||)\n",
    "    - оставляем topk соседей на item\n",
    "    \"\"\"\n",
    "    Xb = build_x_bin_from_xraw(X_raw_csr) if use_binary else X_raw_csr.tocsr()\n",
    "    Xi = Xb.T.tocsr()  # item×user\n",
    "\n",
    "    norms = np.sqrt(np.asarray(Xi.multiply(Xi).sum(axis=1)).ravel()) + 1e-12\n",
    "\n",
    "    C = (Xi @ Xi.T).tocsr()\n",
    "    C.setdiag(0.0)\n",
    "    C.eliminate_zeros()\n",
    "\n",
    "    C = C.tocoo()\n",
    "    C.data = (C.data / (norms[C.row] * norms[C.col])).astype(np.float32)\n",
    "\n",
    "    S = C.tocsr()\n",
    "    S = topk_sorted_csr(S, topk)\n",
    "    return S\n",
    "\n",
    "def hyperbolic_tifu_recommender_factory(X_tifu_csr, S_hyp_csr, fallback_scores=None):\n",
    "    n_items_local = X_tifu_csr.shape[1]\n",
    "\n",
    "    if fallback_scores is None:\n",
    "        fallback_scores = np.asarray(X_tifu_csr.sum(axis=0)).ravel().astype(np.float32)\n",
    "\n",
    "    def recommend(u, topn=TOPN_RECOMMEND):\n",
    "        sim_users = S_hyp_csr[u]\n",
    "        scores_sparse = sim_users.dot(X_tifu_csr)\n",
    "\n",
    "        if scores_sparse.nnz == 0:\n",
    "            scores = fallback_scores\n",
    "        else:\n",
    "            scores = np.zeros(n_items_local, dtype=np.float32)\n",
    "            scores[scores_sparse.indices] = scores_sparse.data.astype(np.float32)\n",
    "\n",
    "        topn_clipped = min(topn, scores.shape[0])\n",
    "        idx = np.argpartition(-scores, topn_clipped)[:topn_clipped]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "        return idx.tolist()\n",
    "\n",
    "    return recommend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num user edges: 19195608\n"
     ]
    }
   ],
   "source": [
    "def build_user_user_graph(R, tau=2):\n",
    "    W = (R @ R.T).tocoo()\n",
    "    \n",
    "    mask = (W.row != W.col) & (W.data >= tau)\n",
    "    edges = np.vstack([W.row[mask], W.col[mask]]).T\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "user_edges = build_user_user_graph(X_raw, tau=2)\n",
    "print(f\"Num user edges: {len(user_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-5\n",
    "\n",
    "class PoincareEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_items, dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_items, dim)\n",
    "        nn.init.uniform_(self.emb.weight, -1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.project_to_ball(self.emb(idx))\n",
    "    \n",
    "    def project_to_ball(self, x, eps=EPS):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "        max_norm = 1 - eps\n",
    "        return x / norm.clamp_min(EPS) * torch.clamp(norm, max=max_norm)\n",
    "    \n",
    "    def poincare_distance(self, x, y):\n",
    "        x2 = (x * x).sum(dim=-1)\n",
    "        y2 = (y * y).sum(dim=-1)\n",
    "        diff2 = ((x - y) ** 2).sum(dim=-1)\n",
    "        denom = (1 - x2) * (1 - y2)\n",
    "        z = 1 + 2 * diff2 / denom.clamp_min(EPS)\n",
    "        return torch.acosh(z.clamp_min(1 + EPS))\n",
    "    \n",
    "    def sample_negatives(self, batch_size, num_items, K):\n",
    "        return torch.randint(low=0, high=num_items, size=(batch_size, K))\n",
    "    \n",
    "    def poincare_loss(self, i, j, negs):\n",
    "        xi = self.forward(i)\n",
    "        xj = self.forward(j)\n",
    "        xk = self.forward(negs)\n",
    "    \n",
    "        d_pos = self.poincare_distance(xi, xj)\n",
    "        d_neg = self.poincare_distance(xi.unsqueeze(1), xk)\n",
    "    \n",
    "        numerator = torch.exp(-d_pos)\n",
    "        denominator = torch.exp(-d_neg).sum(dim=1)\n",
    "    \n",
    "        loss = -torch.log(numerator / denominator.clamp_min(EPS))\n",
    "        return loss.mean()\n",
    "    \n",
    "    def train_embedding(self, edges, num_items, optimizer, epochs=10, batch_size=256, neg_k=10, device=\"cpu\"):\n",
    "        self.to(device)\n",
    "        edges = torch.tensor(edges, dtype=torch.long, device=device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            perm = torch.randperm(len(edges), device=device)\n",
    "            total_loss = 0.0\n",
    "    \n",
    "            for idx in range(0, len(edges), batch_size):\n",
    "                batch_idx = perm[idx:idx + batch_size]\n",
    "                batch = edges[batch_idx]\n",
    "    \n",
    "                i = batch[:, 0]\n",
    "                j = batch[:, 1]\n",
    "                negs = self.sample_negatives(len(i), num_items, neg_k).to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss = self.poincare_loss(i, j, negs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    self.emb.weight.copy_(self.project_to_ball(self.emb.weight))\n",
    "    \n",
    "                total_loss += loss.item() * len(i)\n",
    "    \n",
    "            print(f\"Epoch {epoch+1}: loss = {total_loss / len(edges):.4f}\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 2.1104\n",
      "Epoch 2: loss = 2.0681\n",
      "Epoch 3: loss = 2.0520\n",
      "Epoch 4: loss = 2.0449\n",
      "Epoch 5: loss = 2.0408\n",
      "Epoch 6: loss = 2.0393\n",
      "Epoch 7: loss = 2.0380\n",
      "Epoch 8: loss = 2.0370\n",
      "Epoch 9: loss = 2.0359\n",
      "Epoch 10: loss = 2.0353\n",
      "Epoch 11: loss = 2.0343\n",
      "Epoch 12: loss = 2.0333\n",
      "Epoch 13: loss = 2.0330\n",
      "Epoch 14: loss = 2.0322\n",
      "Epoch 15: loss = 2.0309\n",
      "Epoch 16: loss = 2.0304\n",
      "Epoch 17: loss = 2.0295\n",
      "Epoch 18: loss = 2.0289\n",
      "Epoch 19: loss = 2.0278\n",
      "Epoch 20: loss = 2.0274\n",
      "Epoch 21: loss = 2.0267\n",
      "Epoch 22: loss = 2.0260\n",
      "Epoch 23: loss = 2.0255\n",
      "Epoch 24: loss = 2.0256\n",
      "Epoch 25: loss = 2.0245\n",
      "Epoch 26: loss = 2.0244\n",
      "Epoch 27: loss = 2.0237\n",
      "Epoch 28: loss = 2.0233\n",
      "Epoch 29: loss = 2.0230\n",
      "Epoch 30: loss = 2.0225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PoincareEmbedding(\n",
       "  (emb): Embedding(14074, 30)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = X_raw.shape[0]\n",
    "dim = 30\n",
    "lr = 0.001\n",
    "\n",
    "user_model = PoincareEmbedding(num_users, dim)\n",
    "user_optimizer = torch.optim.Adam(user_model.parameters(), lr=lr)\n",
    "\n",
    "user_model.train_embedding(\n",
    "    edges=user_edges,\n",
    "    num_items=num_users,  \n",
    "    optimizer=user_optimizer,\n",
    "    epochs=30,             \n",
    "    batch_size=1024,\n",
    "    neg_k=15,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poincare_dist_matrix(emb_weight):\n",
    "    norm_sq = (emb_weight ** 2).sum(dim=1, keepdim=True)\n",
    "    dist_sq = norm_sq + norm_sq.t() - 2 * (emb_weight @ emb_weight.t())\n",
    "    \n",
    "    denom = (1 - norm_sq) @ (1 - norm_sq).t()\n",
    "    denom = torch.clamp(denom, min=1e-5)\n",
    "    \n",
    "    arg = 1 + 2 * dist_sq / denom\n",
    "    return torch.acosh(torch.clamp(arg, min=1.0 + 1e-5))\n",
    "\n",
    "def build_user_hyperbolic_knn(model, topk=300, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights = model.emb.weight.data.to(device)\n",
    "        n_users = weights.shape[0]\n",
    "        \n",
    "        dists = poincare_dist_matrix(weights)\n",
    "        \n",
    "        sims = torch.exp(-dists)\n",
    "        \n",
    "        sims.fill_diagonal_(0)\n",
    "        \n",
    "        vals, inds = torch.topk(sims, k=topk, dim=1)\n",
    "        \n",
    "        vals = vals.cpu().numpy().flatten()\n",
    "        inds = inds.cpu().numpy().flatten()\n",
    "        \n",
    "        rows = np.repeat(np.arange(n_users), topk)\n",
    "        \n",
    "        S_hyp = sparse.csr_matrix((vals, (rows, inds)), shape=(n_users, n_users), dtype=np.float32)\n",
    "        return S_hyp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "S_hyp_user = build_user_hyperbolic_knn(user_model, topk=1000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_hyp_tifu = hyperbolic_tifu_recommender_factory(X_tifu, S_hyp_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(pred_items, true_items, k):\n",
    "    \"\"\"\n",
    "    Recall@K = доля товаров из истинной корзины, которые попали в top-K рекомендаций.\n",
    "    true_items: список товаров в истинной корзине (val/test)\n",
    "    pred_items: ранжированный список рекомендаций\n",
    "    \"\"\"\n",
    "    pred_k = pred_items[:k]\n",
    "    true_set = set(true_items)\n",
    "    if len(true_set) == 0:\n",
    "        return 0.0\n",
    "    return len(set(pred_k) & true_set) / len(true_set)\n",
    "\n",
    "def ndcg_at_k(pred_items, true_items, k):\n",
    "    \"\"\"\n",
    "    NDCG@K учитывает порядок: попадания в верхние позиции оцениваются выше.\n",
    "    Здесь релевантность бинарная: товар релевантен, если он есть в true_items.\n",
    "    \"\"\"\n",
    "    true_set = set(true_items)\n",
    "    pred_k = pred_items[:k]\n",
    "\n",
    "    # DCG\n",
    "    dcg = 0.0\n",
    "    for i, it in enumerate(pred_k):\n",
    "        if it in true_set:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # i=0 -> log2(2)=1\n",
    "\n",
    "    # IDCG: максимум возможного DCG при идеальном ранжировании\n",
    "    ideal_hits = min(k, len(true_set))\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate_model(recommender_fn, users, true_baskets, topk_list=(5,10,20)):\n",
    "    \"\"\"\n",
    "    Оцениваем модель на пользователях:\n",
    "    - recommender_fn(u) должен возвращать ранжированный список item_id (индексы товаров)\n",
    "    - true_baskets[u] — истинная корзина (список item_id)\n",
    "    Возвращаем средние Recall@K и NDCG@K по пользователям для каждого K.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for u in users:\n",
    "        u = int(u)\n",
    "        pred = recommender_fn(u)\n",
    "        true = true_baskets[u]\n",
    "        for k in topk_list:\n",
    "            rows.append({\n",
    "                \"u\": u,\n",
    "                \"k\": int(k),\n",
    "                \"recall\": recall_at_k(pred, true, k),\n",
    "                \"ndcg\": ndcg_at_k(pred, true, k),\n",
    "            })\n",
    "\n",
    "    return (pd.DataFrame(rows)\n",
    "            .groupby(\"k\")[[\"recall\",\"ndcg\"]].mean()\n",
    "            .reset_index())\n",
    "\n",
    "def tag_result(df_res, model_name, split_name):\n",
    "    \"\"\"Добавляем метаданные (название модели и сплит) к таблице метрик.\"\"\"\n",
    "    out = df_res.copy()\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"split\"] = split_name\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>recall</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.052538</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.063113</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.089115</td>\n",
       "      <td>0.069009</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k    recall      ndcg                      model split\n",
       "0   5  0.052538  0.062253  TIFU-KNN_Hyper(topk=1000)   val\n",
       "1  10  0.068223  0.063113  TIFU-KNN_Hyper(topk=1000)   val\n",
       "2  20  0.089115  0.069009  TIFU-KNN_Hyper(topk=1000)   val"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "users_val = list(val_basket.keys())\n",
    "res_userknn_hyper_val = evaluate_model(\n",
    "    lambda u: rec_hyp_tifu(u, TOPN_RECOMMEND),\n",
    "    users_val,\n",
    "    val_basket,\n",
    "    TOPK_LIST\n",
    ")\n",
    "\n",
    "val_table_user = pd.concat([\n",
    "    tag_result(res_userknn_hyper_val, f\"TIFU-KNN_Hyper(topk={1000})\", \"val\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "val_table_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>recall</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.073137</td>\n",
       "      <td>0.087003</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.093088</td>\n",
       "      <td>0.088212</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.114282</td>\n",
       "      <td>0.093691</td>\n",
       "      <td>TIFU-KNN_Hyper(topk=1000)</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k    recall      ndcg                      model split\n",
       "0   5  0.073137  0.087003  TIFU-KNN_Hyper(topk=1000)  test\n",
       "1  10  0.093088  0.088212  TIFU-KNN_Hyper(topk=1000)  test\n",
       "2  20  0.114282  0.093691  TIFU-KNN_Hyper(topk=1000)  test"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_test = list(test_basket.keys())\n",
    "res_userknn_hyper_test = evaluate_model(\n",
    "    lambda u: rec_hyp_tifu(u, TOPN_RECOMMEND),\n",
    "    users_test,\n",
    "    test_basket,\n",
    "    TOPK_LIST\n",
    ")\n",
    "\n",
    "test_table_user = pd.concat([\n",
    "    tag_result(res_userknn_hyper_test, f\"TIFU-KNN_Hyper(topk={1000})\", \"test\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "test_table_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 22987,
     "sourceId": 29446,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
