{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgNBiXhtBGr5"
      },
      "source": [
        "# Презентация KNN\n",
        "---\n",
        "\n",
        "## Слайд 1\n",
        "\n",
        "Вступительный слайд\n",
        "\n",
        "## Слайд 2. Тема, цель, гипотеза\n",
        "\n",
        "Доброе утро !\n",
        "\n",
        "Сегодня мы рады представить результаты по работе над проектом **Hyperbolic kNN**.\n",
        "\n",
        "На курсе была поставлена задача: проверить, как меняется качество kNN, если заменить стандартную евклидову геометрию и косинусную близость на **гиперболическое пространство** и гиперболическую метрику.\n",
        "\n",
        "Мотивация задачи следующая: гиперболические пространства хорошо описывают **иерархии** и структуры вида дерева. В покупках часто можно видеть иерархические шаблоны: категория → подкатегория → товар, и пользовательские предпочтения тоже могут иметь подобную структуру.\n",
        "\n",
        "Из мотивации была выдвинута гипотеза: если kNN измеряет похожесть в геометрии, которая лучше соответствует структуре данных, то качество рекомендаций может улучшиться.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 3. Данные и постановка задачи\n",
        "\n",
        "Итак, мы решаем задачу **next-basket recommendation**: у каждого пользователя есть история покупок, и по этой истории нужно предсказать товары следующей корзины.\n",
        "\n",
        "Используем датасет **Ta-Feng** — транзакции супермаркета. Транзакции агрегируем в **дневные корзины**: для каждого пользователя и дня получаем множество уникальных товаров, купленных в этот день.\n",
        "\n",
        "Так для каждого пользователя получается временная последовательность корзин.\n",
        "\n",
        "Оставляем только пользователей с хотя бы тремя корзинами, чтобы корректно построить train, validation и test.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 4. Эксперименты и метрики\n",
        "\n",
        "Важно недопустить учтечки данных в будущее, поэтому мы используем **time-based split per user**: сортируем корзины по времени и делим так: train — все корзины, кроме двух последних; validation — предпоследняя; test — последняя. Любые матрицы взаимодействий и статистики строятся **только по train**.\n",
        "\n",
        "Метрики:\n",
        "\n",
        "- **Recall@K** — доля истинной корзины, попавшая в топ-K рекомендаций,\n",
        "- **NDCG@K** — качество ранжирования, то есть насколько высоко стоят релевантные товары,\n",
        "- **Coverage@K** — доля каталога, которая появляется в рекомендациях по всем пользователям.\n",
        "\n",
        "Отчётные значения K: **10 и 100**, чтобы видеть поведение на небольшой выборке и на более длинном списке.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 5. Бейзлайны\n",
        "\n",
        "Чтобы честно оценить гиперболический kNN, мы построили ряд базовых моделей:\n",
        "\n",
        "1. **TopPop** — рекомендации самых популярных товаров в train. Это нижняя граница и проверка силы популярности в датасете.\n",
        "2. **UserKNN (cosine)** — основной baseline. Именно его мы затем гиперболизируем.\n",
        "3. **TIFU-KNN (simple)** — time-aware kNN: строим два профиля: IU (item usage) — бинарный “встречался ли товар”, PIF (Products Interaction Frequency) — частоты с временным затуханием по корзинам. Похожесть — смесь косинусов PIF и IU с коэффициентом alpha.\n",
        "4. **SVD (TruncatedSVD)** — быстрый baseline, основанный на матричной факторизации.\n",
        "5. **ItemKNN (cosine)** — item-based вариант kNN. Помогает более широко посмотреть на модели KNN.\n",
        "\n",
        "Гиперпараметры тюнили на подмножестве пользователей по **NDCG@10**, затем фиксировали лучшие и оценивали на полных валидации и тесте.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 6. Гиперболическое пространство\n",
        "\n",
        "Теперь про гиперболические варианты. Мы реализовали **Hyperbolic ItemKNN**, **Hyperbolic UserKNN** и **Hyperbolic TIFU-KNN** в модели Пуанкаре: считаем гиперболическое расстояние и преобразуем его в похожесть. Пробовали три преобразования: Экспоненциальное (exp(−d)), Инверсионное (1/(1+d)) и гауссовское.\n",
        "\n",
        "В текущей реализации гиперболический KNN **не превзошёл** бейзлайны с евклидовой геометрией по NDCG/Recall. Это важный результат, который говорит нам, что замена геометрии не гарантирует улучшений. Мы также видим, что выбор функции similarity сильно влияет на coverage: некоторые варианты расширяют покрытие каталога, но качество падает.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 7. Гиперболические эмбеддинги\n",
        "\n",
        "В методе poincare_loss реализуется контрастивная потеря с негативным сэмплированием, аналогичная Noise Contrastive Estimation (NCE) или InfoNCE:\n",
        "Для каждой положительной пары (i, j) и K негативных элементов (negs):\n",
        "$$\\ell = -\\log\\left[ \\frac{\\exp(-d(i,j))}{\\exp(-d(i,j)) + \\sum_k \\exp(-d(i,k))} \\right]$$\n",
        "\n",
        "Есть важный нюанс: эмбеддинги должны лежать внутри шара Пуанкаре радиуса 1, т.е. их норма должна быть строго меньше 1.\n",
        "Поэтому после каждого шага оптимизатора применяется проекция на шар:\n",
        "$$\n",
        "\\tilde{x} = \n",
        "\\begin{cases} \n",
        "x & \\text{если } \\|x\\| < 1 - \\epsilon \\\\\n",
        "\\frac{x}{\\|x\\|} \\cdot (1 - \\epsilon) & \\text{если } \\|x\\| \\geq 1 - \\epsilon\n",
        "\\end{cases}\n",
        "\n",
        "$$\n",
        "где:\n",
        "- ||x|| — евклидова норма вектора (x) (L2-норма),\n",
        "- $\\epsilon$ — маленькая положительная константа (в коде `EPS = 1e-5`), введённая для численной стабильности и чтобы избежать выхода на границу шара (расстояние Пуанкаре определено только строго внутри шара).\n",
        "\n",
        "Мы минимизируем среднюю по всем положительным парам контрастивную потерю:\n",
        "$$L = \\frac{1}{|E|} \\sum_{(i,j) \\in E} -\\log\\left[ \\frac{\\exp(-d(i,j))}{\\exp(-d(i,j)) + \\sum_k \\exp(-d(i,k))} \\right]$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 8. Результаты экспериментов\n",
        "\n",
        "По результатам на **TEST** лидеры по качеству — **UserKNN** и **TIFU-KNN**.\n",
        "\n",
        "Если смотреть на ранжирование небольшого числа товаров, **UserKNN** лучший по NDCG@10, у TIFU эта метрика чуть ниже. Если смотреть на Recall, TIFU немного лучше. На K=100 сохраняется та же картина: UserKNN лучше по NDCG@100, TIFU — чуть лучше по Recall@100. Разница небольшая, но стабильная.\n",
        "\n",
        "Сравнение TopPop и SVD показывает, что в этих данных большое значение имеет популярность: популярностная модель на test сопоставима с SVD по NDCG@10 и лучше при оценки топ-100 товаров в корзине.\n",
        "\n",
        "Теперь немного про **coverage**.\n",
        "Популярностная модель почти не покрывает каталог, так как выдаёт фиксированный набор. SVD увеличивает coverage, но не сильно. UserKNN и TIFU дают существенно более высокий coverage при хорошем качестве — порядка половины каталога при K=100. ItemKNN почти покрывает весь каталог: coverage около **0.91** при K=100, но при этом сильно уступает по NDCG и Recall. Это демонстрирует компромисс между разнообразием и точностью.\n",
        "\n",
        "---\n",
        "\n",
        "## Слайд 9. Заключение\n",
        "\n",
        "В ходе проекта мы построили воспроизводимый протокол next-basket рекомендаций на Ta-Feng с корректным time-based split и набором сильных бейзлайнов, включая метрики Recall@K, NDCG@K и Coverage@K для K = 10 и 100.\n",
        "\n",
        "По качеству на тесте лучшими оказались **UserKNN** и **TIFU-KNN**: UserKNN немного выигрывает по NDCG, а TIFU — по Recall и coverage, что отражает ожидаемый компромисс между ранжированием верхушки и более широким покрытием релевантных товаров.\n",
        "\n",
        "Гиперболические варианты в текущей реализации не превзошли евклидовые бейзлайны, что показывает: простая замена метрики сама по себе не гарантирует улучшений. Основное направление дальнейшей работы — переход к **обучаемым гиперболическим эмбеддингам** и более тщательная настройка функции similarity/параметров геометрии, чтобы гиперболическое пространство реально отражало структуру данных.\n",
        "\n",
        "\n",
        "## Слайд 10.\n",
        "\n",
        "Спасибо за внимание ! \n",
        "Будем рады ответить на вопросы !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
