{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29446,"sourceType":"datasetVersion","datasetId":22987}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport scipy.sparse as sp\nfrom tqdm import tqdm\n\nEPS = 1e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:23.176238Z","iopub.execute_input":"2025-12-22T19:01:23.176475Z","iopub.status.idle":"2025-12-22T19:01:28.316458Z","shell.execute_reply.started":"2025-12-22T19:01:23.176441Z","shell.execute_reply":"2025-12-22T19:01:28.313247Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Импорты и глобальная конфигурация\nimport os, glob, math, pickle, time\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import sparse\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import TruncatedSVD\n\nimport matplotlib.pyplot as plt\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# --- Fast dev mode ---\nDEV_MODE = True\nMAX_USERS = 8000 if DEV_MODE else None  # None для полного прогона\n\nMIN_BASKETS_PER_USER = 3  # должно быть >= 3, чтобы можно было сделать train/val/test\n\n# ---Метрики и размер списка рекомендаций---\nTOPK_LIST = [5, 10, 20]\nTOPN_RECOMMEND = 200  # внутренняя длина списка кандидатов (сколько объектов ранжируем)\n\n\n# --- UserKNN ---\nUSERKNN_TUNE_GRID = [50, 100, 200, 500]  # значения числа соседей для подбора\nUSERKNN_DEFAULT_K = 200\n\n# --- TIFU-KNN(simple) ---\nTIFU_GROUPS_GRID = [5, 7]              # варианты числа групп истории\nTIFU_ALPHA_GRID = [0.5, 0.7, 0.9]      # варианты смешивания PIF/IU\nTIFU_NEIGHBORS_GRID = [100, 300]       # варианты числа соседей\n\n# --- ItemKNN (добавляем, чтобы \"KNN baseline\" был однозначно покрыт) ---\nITEMKNN_TUNE_GRID = [50, 100, 200]   # сколько похожих items хранить на item (topK)\nITEMKNN_DEFAULT_K = 100\n\n\nTIFU_WITHIN_DECAY = 0.9  # затухание внутри группы\nTIFU_GROUP_DECAY = 0.7   # затухание между группами\nTIFU_DEFAULT_GROUPS = 7\nTIFU_DEFAULT_ALPHA = 0.7\nTIFU_DEFAULT_K = 300\n\nprint(\"DEV_MODE:\", DEV_MODE, \"MAX_USERS:\", MAX_USERS)\n\ndef find_csv_candidate():\n    # Ищем CSV-файлы в папке Kaggle input\n    cands = glob.glob('/kaggle/input/*/*.csv') + glob.glob('/kaggle/input/*/*.CSV')\n    if not cands:\n        raise FileNotFoundError('В /kaggle/input не найдены CSV. Проверьте, что датасет добавлен в ноутбук.')\n\n    # Предпочитаем файл, который похож на Ta-Feng по названию\n    for p in cands:\n        low = p.lower()\n        if ('ta' in low and 'feng' in low) or ('tafeng' in low):\n            return p\n\n    # Если не нашли — берём самый большой CSV (как запасной вариант)\n    cands = sorted(cands, key=lambda p: os.path.getsize(p), reverse=True)\n    return cands[0]\n\ndef detect_columns(df: pd.DataFrame):\n    cols = {c.lower(): c for c in df.columns}\n\n    def pick(candidates):\n        for c in candidates:\n            if c in cols:\n                return cols[c]\n        return None\n\n    user_col = pick(['customer_id', 'cust_id', 'user_id', 'userid', 'member_id', 'client_id'])\n    item_col = pick(['product_id', 'item_id', 'prod_id', 'sku_id', 'article_id'])\n    date_col = pick(['transaction_dt', 'trans_date', 'date', 't_dat', 'datetime', 'transaction_date'])\n\n    # В Ta-Feng часто встречаются имена в верхнем регистре: CUSTOMER_ID, PRODUCT_ID, TRANSACTION_DT\n    if user_col is None:\n        for c in df.columns:\n            if c.upper() == 'CUSTOMER_ID':\n                user_col = c\n                break\n    if item_col is None:\n        for c in df.columns:\n            if c.upper() == 'PRODUCT_ID':\n                item_col = c\n                break\n    if date_col is None:\n        for c in df.columns:\n            if c.upper() == 'TRANSACTION_DT':\n                date_col = c\n                break\n\n    return user_col, item_col, date_col\n\npath = find_csv_candidate()\nprint(\"Using CSV:\", path)\n\ndf = pd.read_csv(path)\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", list(df.columns)[:30])\n\nuser_col, item_col, date_col = detect_columns(df)\nprint(\"Detected columns:\", {\"user_col\": user_col, \"item_col\": item_col, \"date_col\": date_col})\n\nif user_col is None or item_col is None or date_col is None:\n    raise ValueError(\n        \"Не удалось автоматически определить необходимые колонки. \"\n        \"Пожалуйста, задайте user_col/item_col/date_col вручную после просмотра df.columns.\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:28.317996Z","iopub.execute_input":"2025-12-22T19:01:28.320563Z","iopub.status.idle":"2025-12-22T19:01:30.727941Z","shell.execute_reply.started":"2025-12-22T19:01:28.320527Z","shell.execute_reply":"2025-12-22T19:01:30.727268Z"}},"outputs":[{"name":"stdout","text":"DEV_MODE: True MAX_USERS: 8000\nUsing CSV: /kaggle/input/ta-feng-grocery-dataset/ta_feng_all_months_merged.csv\nShape: (817741, 9)\nColumns: ['TRANSACTION_DT', 'CUSTOMER_ID', 'AGE_GROUP', 'PIN_CODE', 'PRODUCT_SUBCLASS', 'PRODUCT_ID', 'AMOUNT', 'ASSET', 'SALES_PRICE']\nDetected columns: {'user_col': 'CUSTOMER_ID', 'item_col': 'PRODUCT_ID', 'date_col': 'TRANSACTION_DT'}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Приводим типы к строкам (важно, чтобы не потерять ведущие нули в идентификаторах)\ndf[user_col] = df[user_col].astype(str)\ndf[item_col] = df[item_col].astype(str)\n\n# Парсим дату, некорректные строки превращаются в NaT\ndf[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n\n# Удаляем строки без даты/пользователя/товара\ndf = df.dropna(subset=[date_col, user_col, item_col]).copy()\n\n# Округляем datetime вниз до даты (без времени)\ndf['__date'] = df[date_col].dt.floor('D')\n\n# Группировка в корзины\nbasket_df = (\n    df.groupby([user_col, '__date'])[item_col]\n      .apply(lambda s: list(pd.unique(s)))   # уникальные товары в корзине\n      .reset_index()\n      .rename(columns={user_col: 'user_raw', '__date': 'date', item_col: 'items_raw'})\n)\n\nbasket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\nprint(\"Baskets:\", basket_df.shape, \"Unique users:\", basket_df['user_raw'].nunique())\nbasket_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:30.728873Z","iopub.execute_input":"2025-12-22T19:01:30.729162Z","iopub.status.idle":"2025-12-22T19:01:37.507181Z","shell.execute_reply.started":"2025-12-22T19:01:30.729137Z","shell.execute_reply":"2025-12-22T19:01:37.506557Z"}},"outputs":[{"name":"stdout","text":"Baskets: (119578, 3) Unique users: 32266\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  user_raw       date                                          items_raw\n0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...\n1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...\n2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...\n3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...\n4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_raw</th>\n      <th>date</th>\n      <th>items_raw</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100021</td>\n      <td>2000-11-03</td>\n      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100021</td>\n      <td>2000-11-05</td>\n      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100021</td>\n      <td>2000-11-19</td>\n      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100021</td>\n      <td>2000-11-28</td>\n      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100021</td>\n      <td>2000-12-02</td>\n      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np\nimport pandas as pd\n\n# Фильтруем пользователей с достаточным числом корзин:\n#    нам нужно минимум 3 корзины на пользователя, чтобы сформировать train/val/test\ncounts = basket_df.groupby('user_raw').size()\nkeep_users = counts[counts >= MIN_BASKETS_PER_USER].index\nbasket_df = basket_df[basket_df['user_raw'].isin(keep_users)].copy()\n\n# Опционально: режим разработки (dev).\n#    Если задан MAX_USERS, берём только первых N пользователей (после сортировки/порядка появления).\n#    Это ускоряет эксперименты и отладку в Kaggle.\nif MAX_USERS is not None:\n    users = basket_df['user_raw'].unique()[:MAX_USERS]\n    basket_df = basket_df[basket_df['user_raw'].isin(users)].copy()\n\n# На всякий случай пересортируем по пользователю и времени,\n#    чтобы дальнейший split по времени был корректным\nbasket_df = basket_df.sort_values(['user_raw', 'date']).reset_index(drop=True)\nprint(\"After filter/dev: baskets\", basket_df.shape, \"users\", basket_df['user_raw'].nunique())\n\n# Маппинг сырого user/item ID в индексы 0..n-1\n#    Это нужно для эффективной работы с матрицами (scipy.sparse) и моделями.\nuser_ids = basket_df['user_raw'].unique()\nitem_ids = pd.unique(np.concatenate(basket_df['items_raw'].values))\n\nuser2idx = {u: i for i, u in enumerate(user_ids)}\nitem2idx = {it: i for i, it in enumerate(item_ids)}\n\n# обратные отображения (удобно для дебага/вывода рекомендаций)\nidx2user = {i: u for u, i in user2idx.items()}\nidx2item = {i: it for it, i in item2idx.items()}\n\n# 5) Добавляем индекс пользователя и переводим списки товаров в список индексов\nbasket_df['u'] = basket_df['user_raw'].map(user2idx)\nbasket_df['item_idx_list'] = basket_df['items_raw'].apply(lambda xs: [item2idx[x] for x in xs])\n\nn_users = len(user2idx)\nn_items = len(item2idx)\nprint(\"n_users:\", n_users, \"n_items:\", n_items)\n\ndisplay(basket_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:37.508245Z","iopub.execute_input":"2025-12-22T19:01:37.508556Z","iopub.status.idle":"2025-12-22T19:01:37.965938Z","shell.execute_reply.started":"2025-12-22T19:01:37.508530Z","shell.execute_reply":"2025-12-22T19:01:37.965311Z"}},"outputs":[{"name":"stdout","text":"After filter/dev: baskets (53734, 3) users 8000\nn_users: 8000 n_items: 21149\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  user_raw       date                                          items_raw  u  \\\n0   100021 2000-11-03  [9310042571491, 4719783004070, 4711049230223, ...  0   \n1   100021 2000-11-05  [4710018004605, 4719111020109, 4710247005299, ...  0   \n2   100021 2000-11-19  [4711686002016, 47106710, 4711686002528, 47102...  0   \n3   100021 2000-11-28  [4711800531385, 4714981010038, 4710339772139, ...  0   \n4   100021 2000-12-02  [4710088436511, 4710094014741, 4710105045443, ...  0   \n\n                                      item_idx_list  \n0                                [0, 1, 2, 3, 4, 5]  \n1              [6, 7, 8, 9, 10, 11, 12, 13, 14, 15]  \n2  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]  \n3      [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]  \n4                              [32, 39, 20, 40, 41]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_raw</th>\n      <th>date</th>\n      <th>items_raw</th>\n      <th>u</th>\n      <th>item_idx_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100021</td>\n      <td>2000-11-03</td>\n      <td>[9310042571491, 4719783004070, 4711049230223, ...</td>\n      <td>0</td>\n      <td>[0, 1, 2, 3, 4, 5]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100021</td>\n      <td>2000-11-05</td>\n      <td>[4710018004605, 4719111020109, 4710247005299, ...</td>\n      <td>0</td>\n      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100021</td>\n      <td>2000-11-19</td>\n      <td>[4711686002016, 47106710, 4711686002528, 47102...</td>\n      <td>0</td>\n      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100021</td>\n      <td>2000-11-28</td>\n      <td>[4711800531385, 4714981010038, 4710339772139, ...</td>\n      <td>0</td>\n      <td>[28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100021</td>\n      <td>2000-12-02</td>\n      <td>[4710088436511, 4710094014741, 4710105045443, ...</td>\n      <td>0</td>\n      <td>[32, 39, 20, 40, 41]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"user_baskets = defaultdict(list)  # u -> list of (date, [items])\nfor row in basket_df.itertuples(index=False):\n    user_baskets[row.u].append((row.date, row.item_idx_list))\n\n# Гарантируем сортировку по времени (вдруг где-то нарушилась)\nfor u in user_baskets:\n    user_baskets[u] = sorted(user_baskets[u], key=lambda x: x[0])\n\ntrain_hist = {}\nval_basket = {}\ntest_basket = {}\n\nfor u, seq in user_baskets.items():\n    baskets = [b for _, b in seq]\n    # На всякий случай проверяем минимальную длину\n    if len(baskets) < 3:\n        continue\n    train_hist[u] = baskets[:-2]   # все корзины, кроме двух последних\n    val_basket[u]  = baskets[-2]   # предпоследняя корзина\n    test_basket[u] = baskets[-1]   # последняя корзина\n\nprint(\"Users with train/val/test:\", len(train_hist))\nassert len(train_hist) > 0, \"No users available after filtering/splitting.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:37.966894Z","iopub.execute_input":"2025-12-22T19:01:37.967145Z","iopub.status.idle":"2025-12-22T19:01:38.288909Z","shell.execute_reply.started":"2025-12-22T19:01:37.967122Z","shell.execute_reply":"2025-12-22T19:01:38.288114Z"}},"outputs":[{"name":"stdout","text":"Users with train/val/test: 8000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Строим user-item матрицу только по train_hist (без val/test), чтобы избежать утечки будущего.\n# X_raw[u, it] = сколько train-корзин пользователя u содержали товар it (presence in basket).\nrows, cols, data = [], [], []\nfor u, baskets in train_hist.items():\n    c = Counter()\n    for b in baskets:\n        for it in set(b):   # presence in basket: учитываем товар один раз на корзину\n            c[it] += 1\n    for it, v in c.items():\n        rows.append(u)\n        cols.append(it)\n        data.append(float(v))\n\nX_raw = sparse.csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n\n# L2-нормировка по пользователям для косинусной похожести:\n# cos(u,v) = dot(X_cos[u], X_cos[v])\nX_cos = normalize(X_raw, norm='l2', axis=1)\n\n# Разреженность матрицы: доля ненулевых элементов\ndensity = X_raw.nnz / (n_users * n_items)\nprint(\"X_raw nnz:\", X_raw.nnz, \"density:\", f\"{density:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:38.289990Z","iopub.execute_input":"2025-12-22T19:01:38.290389Z","iopub.status.idle":"2025-12-22T19:01:38.572726Z","shell.execute_reply.started":"2025-12-22T19:01:38.290355Z","shell.execute_reply":"2025-12-22T19:01:38.571987Z"}},"outputs":[{"name":"stdout","text":"X_raw nnz: 226389 density: 0.001338\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def build_item_item_graph(R, tau=2):\n    \"\"\"\n    R: csr_matrix (users x items)\n    tau: threshold\n    \"\"\"\n    # item-item co-occurrence\n    W = (R.T @ R).tocoo()\n    \n    mask = (W.row != W.col) & (W.data >= tau)\n    edges = np.vstack([W.row[mask], W.col[mask]]).T\n    \n    return edges\n\nedges = build_item_item_graph(X_raw, tau=2)\nprint(f\"Num edges: {len(edges)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:38.574644Z","iopub.execute_input":"2025-12-22T19:01:38.574925Z","iopub.status.idle":"2025-12-22T19:01:38.959867Z","shell.execute_reply.started":"2025-12-22T19:01:38.574901Z","shell.execute_reply":"2025-12-22T19:01:38.959259Z"}},"outputs":[{"name":"stdout","text":"Num edges: 3323524\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nEPS = 1e-5\n\nclass PoincareEmbedding(nn.Module):\n    \n    def __init__(self, num_items, dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_items, dim)\n        nn.init.uniform_(self.emb.weight, -1e-3, 1e-3)\n\n    def forward(self, idx):\n        return self.project_to_ball(self.emb(idx))\n    \n    def project_to_ball(self, x, eps=EPS):\n        norm = torch.norm(x, dim=-1, keepdim=True)\n        max_norm = 1 - eps\n        return x / norm.clamp_min(EPS) * torch.clamp(norm, max=max_norm)\n    \n    def poincare_distance(self, x, y):\n        x2 = (x * x).sum(dim=-1)\n        y2 = (y * y).sum(dim=-1)\n        diff2 = ((x - y) ** 2).sum(dim=-1)\n        denom = (1 - x2) * (1 - y2)\n        z = 1 + 2 * diff2 / denom.clamp_min(EPS)\n        return torch.acosh(z.clamp_min(1 + EPS))\n    \n    def sample_negatives(self, batch_size, num_items, K):\n        return torch.randint(low=0, high=num_items, size=(batch_size, K))\n    \n    def poincare_loss(self, i, j, negs):\n        xi = self.forward(i)\n        xj = self.forward(j)\n        xk = self.forward(negs)\n    \n        d_pos = self.poincare_distance(xi, xj)\n        d_neg = self.poincare_distance(xi.unsqueeze(1), xk)\n    \n        numerator = torch.exp(-d_pos)\n        denominator = torch.exp(-d_neg).sum(dim=1)\n    \n        loss = -torch.log(numerator / denominator.clamp_min(EPS))\n        return loss.mean()\n    \n    def train_embedding(self, edges, num_items, optimizer, epochs=10, batch_size=256, neg_k=10, device=\"cpu\"):\n        self.to(device)\n        edges = torch.tensor(edges, dtype=torch.long, device=device)\n        \n        for epoch in range(epochs):\n            perm = torch.randperm(len(edges), device=device)\n            total_loss = 0.0\n    \n            for idx in range(0, len(edges), batch_size):\n                batch_idx = perm[idx:idx + batch_size]\n                batch = edges[batch_idx]\n    \n                i = batch[:, 0]\n                j = batch[:, 1]\n                negs = self.sample_negatives(len(i), num_items, neg_k).to(device)\n    \n                optimizer.zero_grad()\n                loss = self.poincare_loss(i, j, negs)\n                loss.backward()\n                optimizer.step()\n    \n                with torch.no_grad():\n                    self.emb.weight.copy_(self.project_to_ball(self.emb.weight))\n    \n                total_loss += loss.item() * len(i)\n    \n            print(f\"Epoch {epoch+1}: loss = {total_loss / len(edges):.4f}\")\n        \n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:01:38.960777Z","iopub.execute_input":"2025-12-22T19:01:38.961003Z","iopub.status.idle":"2025-12-22T19:01:38.977069Z","shell.execute_reply.started":"2025-12-22T19:01:38.960982Z","shell.execute_reply":"2025-12-22T19:01:38.976391Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"num_items = X_raw.shape[1]\ndim = 20\nlr=0.001\n\nmodel = PoincareEmbedding(num_items, dim)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nmodel.train_embedding(\n    edges=edges,\n    num_items=num_items,\n    optimizer=optimizer,\n    epochs=10,\n    batch_size=1024,\n    neg_k=5,\n    device='cuda'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:09:39.624262Z","iopub.execute_input":"2025-12-22T19:09:39.624597Z","iopub.status.idle":"2025-12-22T19:11:53.250330Z","shell.execute_reply.started":"2025-12-22T19:09:39.624569Z","shell.execute_reply":"2025-12-22T19:11:53.249663Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: loss = 0.4124\nEpoch 2: loss = 0.1638\nEpoch 3: loss = -0.0170\nEpoch 4: loss = -0.0232\nEpoch 5: loss = -0.0236\nEpoch 6: loss = -0.0943\nEpoch 7: loss = -0.1005\nEpoch 8: loss = -0.0927\nEpoch 9: loss = -0.1171\nEpoch 10: loss = -0.1259\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"PoincareEmbedding(\n  (emb): Embedding(21149, 20)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def topk_sorted_csr(mat_csr, k):\n    \"\"\"\n    Оставляет top-k элементов по значению в каждой строке CSR и сортирует их по убыванию.\n    \"\"\"\n    mat = mat_csr.tocsr()\n    indptr, indices, data = mat.indptr, mat.indices, mat.data\n\n    new_indptr = np.zeros(mat.shape[0] + 1, dtype=np.int32)\n    new_indices = []\n    new_data = []\n\n    nnz_so_far = 0\n    for i in range(mat.shape[0]):\n        start, end = indptr[i], indptr[i + 1]\n        row_idx = indices[start:end]\n        row_data = data[start:end]\n\n        if row_data.size == 0:\n            new_indptr[i + 1] = nnz_so_far\n            continue\n\n        if row_data.size > k:\n            top = np.argpartition(-row_data, k)[:k]\n            top = top[np.argsort(-row_data[top])]\n            row_idx = row_idx[top]\n            row_data = row_data[top]\n        else:\n            order = np.argsort(-row_data)\n            row_idx = row_idx[order]\n            row_data = row_data[order]\n\n        new_indices.extend(row_idx.tolist())\n        new_data.extend(row_data.astype(np.float32).tolist())\n        nnz_so_far += len(row_idx)\n        new_indptr[i + 1] = nnz_so_far\n\n    return sparse.csr_matrix(\n        (np.array(new_data, dtype=np.float32),\n         np.array(new_indices, dtype=np.int32),\n         new_indptr),\n        shape=mat.shape\n    )\n\n\ndef build_item_poincare_sim_topk(model, topk=100, version='v1', device='cpu'):\n    \"\"\"\n    Build item-item similarity matrix based on Poincare distances from embeddings.\n    - Compute pairwise Poincare distances between all item embeddings.\n    - Convert distances to similarities (two versions).\n    - Keep top-k neighbors per item, sorted by decreasing similarity.\n    \n    Args:\n    - model: Trained PoincareEmbedding model.\n    - topk: Number of nearest neighbors to keep.\n    - version: 'v1' for sim = exp(-dist), 'v2' for sim = 1 / (dist + 1).\n    - device: Torch device for computation.\n    \n    Returns:\n    - S: CSR matrix (item x item) with top-k similarities.\n    \"\"\"\n    model.eval()\n    model.to(device)\n    \n    with torch.no_grad():\n        items = torch.arange(num_items, device=device)\n        emb = model.forward(items)  # (num_items, dim)\n    \n    # Compute pairwise distances\n    # To avoid OOM, compute in batches if num_items is large\n    batch_size = 1024  # Adjust based on memory\n    dist_matrix = torch.zeros((num_items, num_items), dtype=torch.float32, device=device)\n    \n    for i in range(0, num_items, batch_size):\n        end_i = min(i + batch_size, num_items)\n        xi = emb[i:end_i]\n        \n        x2_i = (xi ** 2).sum(dim=-1, keepdim=True)  # (bs, 1)\n        \n        for j in range(0, num_items, batch_size):\n            end_j = min(j + batch_size, num_items)\n            xj = emb[j:end_j]\n            \n            x2_j = (xj ** 2).sum(dim=-1)  # (bs_j,)\n            diff2 = ((xi.unsqueeze(1) - xj.unsqueeze(0)) ** 2).sum(dim=-1)  # (bs_i, bs_j)\n            denom = (1 - x2_i) * (1 - x2_j.unsqueeze(0))  # (bs_i, bs_j)\n            z = 1 + 2 * diff2 / denom.clamp_min(EPS)\n            dist = torch.acosh(z.clamp_min(1 + EPS))\n            \n            dist_matrix[i:end_i, j:end_j] = dist\n    \n    # Set diagonal to inf (to exclude self)\n    dist_matrix.fill_diagonal_(float('inf'))\n    \n    # Convert to similarity\n    if version == 'exp':\n        sim_matrix = torch.exp(-dist_matrix)  # exp(-dist): from ~1 (close) to 0 (far)\n    elif version == 'inv':\n        sim_matrix = 1 / (dist_matrix + 1)  # 1/(dist+1): from ~1 (close) to 0 (far)\n    elif version == 'gauss':\n        sigma = 1.0\n        sim_matrix = torch.exp(-dist_matrix.pow(2) / (2 * sigma ** 2))\n    elif version == 'norm_inv':\n        max_dist = dist_matrix.max().item()  # или фиксированное значение\n        sim_matrix = 1 / ( (dist_matrix / max_dist) + 1 )\n    else:\n        raise ValueError(\"Invalid sim function\")\n    \n    # Build COO from sim_matrix (non-zero everywhere, but we'll take topk)\n    # To save memory, directly find topk without full matrix\n    # But for simplicity, since num_items might not be too large, proceed\n    rows, cols = torch.nonzero(sim_matrix > 0, as_tuple=True)  # All, since sim >0\n    data = sim_matrix[rows, cols].cpu().numpy()\n    \n    S = sparse.coo_matrix((data, (rows.cpu().numpy(), cols.cpu().numpy())), shape=(num_items, num_items))\n    S = S.tocsr()\n    S = topk_sorted_csr(S, topk)  # Use the existing topk function\n    return S\n\n\ndef itemknn_recommender_factory(X_user_csr, S_itemitem_csr, fallback_scores=None):\n    \"\"\"\n    score(u,:) = X_user[u] @ S_itemitem\n    Важно: результат dot(...) — sparse. Для topN строим плотный вектор корректно.\n    \"\"\"\n    n_items_local = S_itemitem_csr.shape[0]\n\n    # fallback лучше брать как item popularity по train, а не sum(S)\n    if fallback_scores is None:\n        fallback_scores = np.asarray(X_user_csr.sum(axis=0)).ravel().astype(np.float32)\n    else:\n        fallback_scores = np.asarray(fallback_scores).ravel().astype(np.float32)\n\n    def recommend(u, topn=TOPN_RECOMMEND):\n        x = X_user_csr[u]                 # 1×n_items (CSR)\n        tmp = x.dot(S_itemitem_csr)       # 1×n_items (sparse)\n\n        # корректная проверка \"всё нулевое\" для sparse\n        if tmp.nnz == 0:\n            scores = fallback_scores\n        else:\n            # делаем плотный вектор только по ненулевым индексам (быстрее, чем toarray())\n            tmp = tmp.tocsr()\n            scores = np.zeros(n_items_local, dtype=np.float32)\n            scores[tmp.indices] = tmp.data.astype(np.float32)\n\n        topn2 = min(topn, scores.shape[0])\n        idx = np.argpartition(-scores, topn2)[:topn2]\n        idx = idx[np.argsort(-scores[idx])]\n        return idx.tolist()\n\n    return recommend","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:19:45.857426Z","iopub.execute_input":"2025-12-22T19:19:45.857805Z","iopub.status.idle":"2025-12-22T19:19:45.875040Z","shell.execute_reply.started":"2025-12-22T19:19:45.857749Z","shell.execute_reply":"2025-12-22T19:19:45.874012Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Build for v1\nS_item_poincare_v1 = build_item_poincare_sim_topk(model, topk=ITEMKNN_DEFAULT_K, version='exp')\nitemknn_poincare_rec_v1 = itemknn_recommender_factory(X_raw, S_item_poincare_v1)\n\n\n# Build for v1\nS_item_poincare_v2 = build_item_poincare_sim_topk(model, topk=ITEMKNN_DEFAULT_K, version='inv')\nitemknn_poincare_rec_v2 = itemknn_recommender_factory(X_raw, S_item_poincare_v2)\n\n\n# Build for v1\nS_item_poincare_v3 = build_item_poincare_sim_topk(model, topk=ITEMKNN_DEFAULT_K, version='gauss')\nitemknn_poincare_rec_v3 = itemknn_recommender_factory(X_raw, S_item_poincare_v3)\n\n\n# Build for v1\nS_item_poincare_v4 = build_item_poincare_sim_topk(model, topk=ITEMKNN_DEFAULT_K, version='norm_inv')\nitemknn_poincare_rec_v4 = itemknn_recommender_factory(X_raw, S_item_poincare_v4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:19:47.341925Z","iopub.execute_input":"2025-12-22T19:19:47.342281Z","iopub.status.idle":"2025-12-22T19:23:48.901681Z","shell.execute_reply.started":"2025-12-22T19:19:47.342253Z","shell.execute_reply":"2025-12-22T19:23:48.900875Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def recall_at_k(pred_items, true_items, k):\n    \"\"\"\n    Recall@K = доля товаров из истинной корзины, которые попали в top-K рекомендаций.\n    true_items: список товаров в истинной корзине (val/test)\n    pred_items: ранжированный список рекомендаций\n    \"\"\"\n    pred_k = pred_items[:k]\n    true_set = set(true_items)\n    if len(true_set) == 0:\n        return 0.0\n    return len(set(pred_k) & true_set) / len(true_set)\n\ndef ndcg_at_k(pred_items, true_items, k):\n    \"\"\"\n    NDCG@K учитывает порядок: попадания в верхние позиции оцениваются выше.\n    Здесь релевантность бинарная: товар релевантен, если он есть в true_items.\n    \"\"\"\n    true_set = set(true_items)\n    pred_k = pred_items[:k]\n\n    # DCG\n    dcg = 0.0\n    for i, it in enumerate(pred_k):\n        if it in true_set:\n            dcg += 1.0 / np.log2(i + 2)  # i=0 -> log2(2)=1\n\n    # IDCG: максимум возможного DCG при идеальном ранжировании\n    ideal_hits = min(k, len(true_set))\n    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_hits))\n\n    return dcg / idcg if idcg > 0 else 0.0\n\ndef evaluate_model(recommender_fn, users, true_baskets, topk_list=(5,10,20)):\n    \"\"\"\n    Оцениваем модель на пользователях:\n    - recommender_fn(u) должен возвращать ранжированный список item_id (индексы товаров)\n    - true_baskets[u] — истинная корзина (список item_id)\n    Возвращаем средние Recall@K и NDCG@K по пользователям для каждого K.\n    \"\"\"\n    rows = []\n    for u in users:\n        u = int(u)\n        pred = recommender_fn(u)\n        true = true_baskets[u]\n        for k in topk_list:\n            rows.append({\n                \"u\": u,\n                \"k\": int(k),\n                \"recall\": recall_at_k(pred, true, k),\n                \"ndcg\": ndcg_at_k(pred, true, k),\n            })\n\n    return (pd.DataFrame(rows)\n            .groupby(\"k\")[[\"recall\",\"ndcg\"]].mean()\n            .reset_index())\n\ndef tag_result(df_res, model_name, split_name):\n    \"\"\"Добавляем метаданные (название модели и сплит) к таблице метрик.\"\"\"\n    out = df_res.copy()\n    out[\"model\"] = model_name\n    out[\"split\"] = split_name\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:23:54.138122Z","iopub.execute_input":"2025-12-22T19:23:54.138668Z","iopub.status.idle":"2025-12-22T19:23:54.147120Z","shell.execute_reply.started":"2025-12-22T19:23:54.138639Z","shell.execute_reply":"2025-12-22T19:23:54.146436Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"users_val = np.array(sorted(val_basket.keys()))\nprint(\"VAL users:\", len(users_val))\n\nres_itemknn_hyper_val_v1 = evaluate_model(\n    lambda u: itemknn_poincare_rec_v1(u, TOPN_RECOMMEND),\n    users_val,\n    val_basket,\n    TOPK_LIST\n)\n\nres_itemknn_hyper_val_v2 = evaluate_model(\n    lambda u: itemknn_poincare_rec_v2(u, TOPN_RECOMMEND),\n    users_val,\n    val_basket,\n    TOPK_LIST\n)\n\nres_itemknn_hyper_val_v3 = evaluate_model(\n    lambda u: itemknn_poincare_rec_v3(u, TOPN_RECOMMEND),\n    users_val,\n    val_basket,\n    TOPK_LIST\n)\n\nres_itemknn_hyper_val_v4 = evaluate_model(\n    lambda u: itemknn_poincare_rec_v4(u, TOPN_RECOMMEND),\n    users_val,\n    val_basket,\n    TOPK_LIST\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:23:56.828519Z","iopub.execute_input":"2025-12-22T19:23:56.828811Z","iopub.status.idle":"2025-12-22T19:24:08.056274Z","shell.execute_reply.started":"2025-12-22T19:23:56.828786Z","shell.execute_reply":"2025-12-22T19:24:08.055627Z"}},"outputs":[{"name":"stdout","text":"VAL users: 8000\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"val_table_default = pd.concat([\n    tag_result(res_itemknn_hyper_val_v1, f\"ItemKNN_exp(topk={ITEMKNN_DEFAULT_K})\", \"val\"),\n    tag_result(res_itemknn_hyper_val_v2, f\"ItemKNN_inv(topk={ITEMKNN_DEFAULT_K})\", \"val\"),\n    tag_result(res_itemknn_hyper_val_v3, f\"ItemKNN_gauss(topk={ITEMKNN_DEFAULT_K})\", \"val\"),\n    tag_result(res_itemknn_hyper_val_v4, f\"ItemKNN_norm_inv(topk={ITEMKNN_DEFAULT_K})\", \"val\"),\n], ignore_index=True)\n\nval_table_default","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:24:10.892474Z","iopub.execute_input":"2025-12-22T19:24:10.892802Z","iopub.status.idle":"2025-12-22T19:24:10.905176Z","shell.execute_reply.started":"2025-12-22T19:24:10.892773Z","shell.execute_reply":"2025-12-22T19:24:10.904464Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"     k    recall      ndcg                       model split\n0    5  0.004594  0.006586       ItemKNN_exp(topk=100)   val\n1   10  0.006704  0.006861       ItemKNN_exp(topk=100)   val\n2   20  0.011328  0.008496       ItemKNN_exp(topk=100)   val\n3    5  0.005242  0.007252       ItemKNN_inv(topk=100)   val\n4   10  0.008845  0.008070       ItemKNN_inv(topk=100)   val\n5   20  0.014281  0.009937       ItemKNN_inv(topk=100)   val\n6    5  0.004564  0.006338     ItemKNN_gauss(topk=100)   val\n7   10  0.007006  0.006796     ItemKNN_gauss(topk=100)   val\n8   20  0.011884  0.008569     ItemKNN_gauss(topk=100)   val\n9    5  0.001167  0.001626  ItemKNN_norm_inv(topk=100)   val\n10  10  0.002375  0.002245  ItemKNN_norm_inv(topk=100)   val\n11  20  0.003789  0.002664  ItemKNN_norm_inv(topk=100)   val","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>k</th>\n      <th>recall</th>\n      <th>ndcg</th>\n      <th>model</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0.004594</td>\n      <td>0.006586</td>\n      <td>ItemKNN_exp(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>0.006704</td>\n      <td>0.006861</td>\n      <td>ItemKNN_exp(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>0.011328</td>\n      <td>0.008496</td>\n      <td>ItemKNN_exp(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>0.005242</td>\n      <td>0.007252</td>\n      <td>ItemKNN_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>0.008845</td>\n      <td>0.008070</td>\n      <td>ItemKNN_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20</td>\n      <td>0.014281</td>\n      <td>0.009937</td>\n      <td>ItemKNN_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>0.004564</td>\n      <td>0.006338</td>\n      <td>ItemKNN_gauss(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>0.007006</td>\n      <td>0.006796</td>\n      <td>ItemKNN_gauss(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>20</td>\n      <td>0.011884</td>\n      <td>0.008569</td>\n      <td>ItemKNN_gauss(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5</td>\n      <td>0.001167</td>\n      <td>0.001626</td>\n      <td>ItemKNN_norm_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.002375</td>\n      <td>0.002245</td>\n      <td>ItemKNN_norm_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>20</td>\n      <td>0.003789</td>\n      <td>0.002664</td>\n      <td>ItemKNN_norm_inv(topk=100)</td>\n      <td>val</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}